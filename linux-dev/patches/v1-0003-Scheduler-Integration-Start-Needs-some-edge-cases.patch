From 474e1a5fc05a0a5650d536db1b4dbf1eb5658703 Mon Sep 17 00:00:00 2001
From: Noah Goldstein <goldstein.w.n@gmail.com>
Date: Fri, 6 May 2022 00:23:06 -0500
Subject: [PATCH v1 3/9] Scheduler Integration - Start (Needs some edge cases +
 optimization)

---
 kernel/sched/core.c     |  7 +++++
 kernel/sched/deadline.c |  4 +--
 kernel/sched/fair.c     | 66 +++++++++++++++++++++++++++++++++++++----
 kernel/sched/idle.c     |  4 +--
 kernel/sched/rt.c       |  4 +--
 5 files changed, 73 insertions(+), 12 deletions(-)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 51efaabac3e4..c4cb88b14e81 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -7,6 +7,7 @@
  *  Copyright (C) 1991-2002  Linus Torvalds
  */
 #include <linux/highmem.h>
+#include <linux/auto-lock.h>
 #include <linux/hrtimer_api.h>
 #include <linux/ktime_api.h>
 #include <linux/sched/signal.h>
@@ -5022,6 +5023,12 @@ static __always_inline struct rq *
 context_switch(struct rq *rq, struct task_struct *prev,
 	       struct task_struct *next, struct rq_flags *rf)
 {
+
+    /* Make sure autolock is disabled before rescheduling. This is
+     * unnecissary at the moment but has little cost and is necessary for
+     * some potential future directs.
+     */
+    auto_lock_sched(next);
 	prepare_task_switch(rq, prev, next);
 
 	/*
diff --git a/kernel/sched/deadline.c b/kernel/sched/deadline.c
index fb4255ae0b2c..e0eff1638de7 100644
--- a/kernel/sched/deadline.c
+++ b/kernel/sched/deadline.c
@@ -15,7 +15,7 @@
  *                    Michael Trimarchi <michael@amarulasolutions.com>,
  *                    Fabio Checconi <fchecconi@gmail.com>
  */
-
+#include <linux/auto-lock-verbose.h>
 static inline struct task_struct *dl_task_of(struct sched_dl_entity *dl_se)
 {
 	return container_of(dl_se, struct task_struct, dl);
@@ -1988,7 +1988,7 @@ static struct task_struct *pick_task_dl(struct rq *rq)
 static struct task_struct *pick_next_task_dl(struct rq *rq)
 {
 	struct task_struct *p;
-
+    LPRINTK_VV("Picking Deadline Task\n");
 	p = pick_task_dl(rq);
 	if (p)
 		set_next_task_dl(rq, p, true);
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index a68482d66535..71de23d20f04 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -1,3 +1,4 @@
+// clang-format off
 // SPDX-License-Identifier: GPL-2.0
 /*
  * Completely Fair Scheduling (CFS) Class (SCHED_NORMAL/SCHED_BATCH)
@@ -45,6 +46,7 @@
 #include <linux/psi.h>
 #include <linux/ratelimit.h>
 #include <linux/task_work.h>
+#include <linux/auto-lock-verbose.h>
 
 #include <asm/switch_to.h>
 
@@ -4557,10 +4559,13 @@ wakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);
  */
 static struct sched_entity *
 pick_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *curr)
+// clang-format on
 {
 	struct sched_entity *left = __pick_first_entity(cfs_rq);
 	struct sched_entity *se;
 
+	LPRINTK_VVVV("Picking next entity: (%d, %d)\n", curr == NULL,
+		     curr == NULL ? -1 : auto_lock_dont_sched_cur);
 	/*
 	 * If curr is set we have to see if its left of the leftmost entity
 	 * still in the tree, provided there was anything in the tree at all.
@@ -4572,29 +4577,78 @@ pick_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *curr)
 
 	/*
 	 * Avoid running the skip buddy, if running something else can
-	 * be done without getting too unfair.
+	 * be done without getting too unfair. If task of se is marked
+     * dont_sched by autolock try and skip it. We put in mediocre
+     * effort.
 	 */
-	if (cfs_rq->skip && cfs_rq->skip == se) {
+	if (se && (cfs_rq->skip == se || auto_lock_dont_sched(task_of(se)))) {
 		struct sched_entity *second;
 
 		if (se == curr) {
 			second = __pick_first_entity(cfs_rq);
+			LPRINTK_VVVV("se == curr\nGrabbing second: (%d, %d)\n",
+				     second == NULL, second == NULL, -1,
+				     auto_lock_dont_sched(taskof(second)));
 		} else {
 			second = __pick_next_entity(se);
-			if (!second || (curr && entity_before(curr, second)))
+			LPRINTK_VVVV("se != curr\nGrabbing second: (%d, %d)\n",
+				     second == NULL, second == NULL, -1,
+				     auto_lock_dont_sched(taskof(second)));
+
+			if (!second || (curr && entity_before(curr, second))) {
 				second = curr;
+				LPRINTK_VVVV("Second set to curr\n");
+			} else if (auto_lock_dont_sched(task_of(second))) {
+                struct sched_entity *next_second;
+				LPRINTK_VVVV(
+					"Trying to find autolock prefered (%d)\n",
+					auto_lock_dont_sched(second));
+				do {
+					/* Naively try and find another task. Maybe a
+                     * max_iter count would be good here but expectation
+                     * is its only a few iterations.
+                     */
+					next_second =
+						__pick_next_entity(next_second);
+
+					if (!next_second) {
+						LPRINTK_VVVV(
+							"Didn't find autolock prefered\n");
+						goto did_not_find_prefered;
+					}
+
+				} while (auto_lock_dont_sched(
+					task_of(next_second)));
+
+				LPRINTK_VVVV("Found autolock prefered\n");
+				second = next_second;
+			did_not_find_prefered:
+			}
 		}
 
-		if (second && wakeup_preempt_entity(second, left) < 1)
-			se = second;
+		/* Noah_Todo: This logic needs to be improved! */
+		if (second) {
+			if (se && auto_lock_dont_sched(task_of(se))) {
+				LPRINTK_VVVV(
+					"Overwritting se with second (A)\n");
+				se = second;
+			} else if ((!auto_lock_dont_sched(task_of(second))) &&
+				   wakeup_preempt_entity(second, left) < 1) {
+				LPRINTK_VVVV(
+					"Overwritting se with second (B)\n");
+				se = second;
+			}
+		}
 	}
 
+	/* Noah_Todo: Add meaningful logic here. */
 	if (cfs_rq->next && wakeup_preempt_entity(cfs_rq->next, left) < 1) {
 		/*
 		 * Someone really wants this to run. If it's not unfair, run it.
 		 */
 		se = cfs_rq->next;
-	} else if (cfs_rq->last && wakeup_preempt_entity(cfs_rq->last, left) < 1) {
+	} else if (cfs_rq->last &&
+		   wakeup_preempt_entity(cfs_rq->last, left) < 1) {
 		/*
 		 * Prefer last buddy, try to return the CPU to a preempted task.
 		 */
diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index ecb0d7052877..4d566455e61d 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -6,7 +6,7 @@
  * (NOTE: these are not related to SCHED_IDLE batch scheduled
  *        tasks which are handled in sched/fair.c )
  */
-
+#include <linux/auto-lock-verbose.h>
 /* Linker adds these: start and end of __cpuidle functions */
 extern char __cpuidle_text_start[], __cpuidle_text_end[];
 
@@ -446,7 +446,7 @@ static struct task_struct *pick_task_idle(struct rq *rq)
 struct task_struct *pick_next_task_idle(struct rq *rq)
 {
 	struct task_struct *next = rq->idle;
-
+    LPRINTK_VVV("Picking Idle Task\n");
 	set_next_task_idle(rq, next, true);
 
 	return next;
diff --git a/kernel/sched/rt.c b/kernel/sched/rt.c
index a32c46889af8..e119658810b9 100644
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@ -3,7 +3,7 @@
  * Real-Time Scheduling Class (mapped to the SCHED_FIFO and SCHED_RR
  * policies)
  */
-
+#include <linux/auto-lock-verbose.h>
 int sched_rr_timeslice = RR_TIMESLICE;
 int sysctl_sched_rr_timeslice = (MSEC_PER_SEC / HZ) * RR_TIMESLICE;
 /* More than 4 hours if BW_SHIFT equals 20. */
@@ -1736,7 +1736,7 @@ static struct task_struct *_pick_next_task_rt(struct rq *rq)
 {
 	struct sched_rt_entity *rt_se;
 	struct rt_rq *rt_rq  = &rq->rt;
-
+    LPRINTK_VV("Picking Real-Time Task\n");
 	do {
 		rt_se = pick_next_rt_entity(rt_rq);
 		BUG_ON(!rt_se);
-- 
2.34.1

