From 67dfd3455744a9239891cca3401ed080702ec5ec Mon Sep 17 00:00:00 2001
From: Noah Goldstein <goldstein.w.n@gmail.com>
Date: Sat, 7 May 2022 14:20:17 -0500
Subject: [PATCH v1 6/6] Factor stats collection out to autolock-verbose and
 start profiling scheduler

---
 include/linux/auto-lock-verbose.h |   60 +-
 include/linux/auto-lock.h         |    4 +
 kernel/auto-lock.c                |   34 +-
 kernel/sched/fair.c               | 1403 +++++++++++++++++++++--------
 4 files changed, 1094 insertions(+), 407 deletions(-)

diff --git a/include/linux/auto-lock-verbose.h b/include/linux/auto-lock-verbose.h
index 019a5473b3b0..98821ef08ba6 100644
--- a/include/linux/auto-lock-verbose.h
+++ b/include/linux/auto-lock-verbose.h
@@ -7,16 +7,70 @@
 #define I_LPRINTK(msg, ...)                                                    \
 	printk("%-20s:%-6u: " msg, __func__, __LINE__, ##__VA_ARGS__)
 
-#define LPRINTK_V(...)  //I_LPRINTK(__VA_ARGS__)
-#define LPRINTK_VV(...)  //I_LPRINTK(__VA_ARGS__)
+#define LPRINTK_V(...) //I_LPRINTK(__VA_ARGS__)
+#define LPRINTK_VV(...) //I_LPRINTK(__VA_ARGS__)
 #define LPRINTK_VVV(...) // I_LPRINTK(__VA_ARGS__)
 #define LPRINTK_VVVV(...) // I_LPRINTK(__VA_ARGS__)
 
 #define I_LTRACE() printk("%-20s:%-6u\n", __func__, __LINE__)
 
-#define LTRACE_V()  //I_LTRACE()
+#define LTRACE_V() //I_LTRACE()
 #define LTRACE_VV() // I_LTRACE()
 #define LTRACE_VVV() // I_LTRACE()
 #define LTRACE_VVVV() // I_LTRACE()
 
+#define I_LV_TO_STR(x) #x
+#define LV_TO_STR(x) I_LV_TO_STR(x)
+#define STATS_V_OUT(x) LV_TO_STR(x), atomic_read(&(x))
+
+//#define WITH_STATS
+#ifdef WITH_STATS
+static atomic_t print_throttle __attribute__((unused));
+
+static atomic_t fast_read_success __attribute__((unused));
+static atomic_t slow_read_success __attribute__((unused));
+static atomic_t both_failure __attribute__((unused));
+
+static atomic_t sched_print_throttle __attribute__((unused));
+static atomic_t total_calls __attribute__((unused));
+static atomic_t out_is_null __attribute__((unused));
+static atomic_t out_dont_sched __attribute__((unused));
+static atomic_t out_has_al_ctx __attribute__((unused));
+static atomic_t out_has_al __attribute__((unused));
+static atomic_t out_is_armed __attribute__((unused));
+
+static atomic_t out_may_sched __attribute__((unused));
+static atomic_t curr_dont_sched __attribute__((unused));
+static atomic_t left_dont_sched __attribute__((unused));
+static atomic_t se_dont_sched __attribute__((unused));
+static atomic_t next_dont_sched __attribute__((unused));
+static atomic_t last_dont_sched __attribute__((unused));
+
+static atomic_t se_skip __attribute__((unused));
+static atomic_t se_eq_curr __attribute__((unused));
+static atomic_t se_neq_curr __attribute__((unused));
+static atomic_t second_eq_curr __attribute__((unused));
+static atomic_t sec_second_dont_sched __attribute__((unused));
+static atomic_t snc_second_dont_sched __attribute__((unused));
+static atomic_t replace_left_curr_dont_sched __attribute__((unused));
+static atomic_t replace_left_left_dont_sched __attribute__((unused));
+
+static atomic_t replace_left __attribute__((unused));
+static atomic_t replace_wakeup_second __attribute__((unused));
+static atomic_t replace_wakeup_next __attribute__((unused));
+static atomic_t replace_wakeup_last __attribute__((unused));
+
+#define stats_add(x, v) atomic_fetch_add(!!(v), x)
+#define stats_inc(x) atomic_inc(x)
+#define stats_print(throttle, ...)                                       \
+	if ((atomic_fetch_add(1, &(throttle)) % 256) == 0) {               \
+		printk(__VA_ARGS__);                                           \
+	}
+
+#else
+#define stats_add(x, v)
+#define stats_inc(x)
+#define stats_print(...)
+#endif
+
 #endif
diff --git a/include/linux/auto-lock.h b/include/linux/auto-lock.h
index a8ba98c6f3b9..f939954fd78d 100644
--- a/include/linux/auto-lock.h
+++ b/include/linux/auto-lock.h
@@ -18,4 +18,8 @@ void auto_lock_free(struct task_struct *tsk);
 /* Called before scheduling a task. */
 void auto_lock_sched(struct task_struct const *tsk);
 
+int auto_lock_exists(struct task_struct const *tsk);
+int auto_lock_has_al(struct task_struct const *tsk);
+int auto_lock_is_armed_s(struct task_struct const *tsk);
+
 #endif
diff --git a/kernel/auto-lock.c b/kernel/auto-lock.c
index 53fe9c27bfa5..503c08837497 100644
--- a/kernel/auto-lock.c
+++ b/kernel/auto-lock.c
@@ -58,24 +58,6 @@ struct auto_lock_ctx {
 #endif
 };
 
-#ifdef WITH_STATS
-static atomic_t print_throttler;
-
-static atomic_t fast_read_success;
-static atomic_t slow_read_success;
-static atomic_t both_failure;
-
-#define stats_inc(x) atomic_inc(x)
-#define stats_print(...)                                                       \
-	if ((atomic_fetch_add(1, &print_throttler) % 256) == 0) {              \
-		printk(__VA_ARGS__);                                           \
-	}
-
-#else
-#define stats_inc(x)
-#define stats_print(...)
-#endif
-
 /* Allocate a page of physical memory. */
 static struct auto_lock *auto_lock_alloc_mem(void)
 {
@@ -168,7 +150,7 @@ static int __auto_lock_check_mem(struct task_struct const *tsk,
 		LPRINTK_VVV("Fast read succeeded\n");
 		stats_inc(&fast_read_success);
 	}
-	stats_print("fast(%u), slow(%u), fail(%u)\n",
+	stats_print(print_throttle, "fast(%u), slow(%u), fail(%u)\n",
 		    atomic_read(&fast_read_success),
 		    atomic_read(&slow_read_success),
 		    atomic_read(&both_failure));
@@ -515,3 +497,17 @@ void auto_lock_sched(struct task_struct const *tsk)
 	LPRINTK_VVV("Dearmining autolock\n");
 	return __auto_lock_dearm(tsk->auto_lock->auto_lock);
 }
+
+int auto_lock_exists(struct task_struct const *tsk)
+{
+	return tsk && tsk->auto_lock != NULL;
+}
+int auto_lock_has_al(struct task_struct const *tsk)
+{
+	return auto_lock_exists(tsk) && tsk->auto_lock->auto_lock != NULL;
+}
+int auto_lock_is_armed_s(struct task_struct const *tsk)
+{
+	return auto_lock_has_al(tsk) &&
+	       __auto_lock_is_armed(tsk->auto_lock->auto_lock);
+}
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 4b202b6808ea..10894d892e70 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -4550,6 +4550,15 @@ set_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)
 static int
 wakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);
 
+/*
+ * Pick the next process, keeping these things in mind, in this order:
+ * 1) keep things fair between processes/task groups
+ * 2) pick the "next" process, since someone really wants that to run
+ * 3) pick the "last" process, for cache locality
+ * 4) do not run the "skip" process, if something else is available
+ */
+#define SCHED_VERSION 5
+#if SCHED_VERSION == 0
 /*
  * Pick the next process, keeping these things in mind, in this order:
  * 1) keep things fair between processes/task groups
@@ -4559,7 +4568,6 @@ wakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se);
  */
 static struct sched_entity *
 pick_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *curr)
-// clang-format on
 {
 	struct sched_entity *left = __pick_first_entity(cfs_rq);
 	struct sched_entity *se;
@@ -4573,6 +4581,56 @@ pick_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *curr)
 
 	se = left; /* ideally we run the leftmost entity */
 
+	/*
+	 * Avoid running the skip buddy, if running something else can
+	 * be done without getting too unfair.
+	 */
+	if (cfs_rq->skip && cfs_rq->skip == se) {
+		struct sched_entity *second;
+
+		if (se == curr) {
+			second = __pick_first_entity(cfs_rq);
+		} else {
+			second = __pick_next_entity(se);
+			if (!second || (curr && entity_before(curr, second)))
+				second = curr;
+		}
+
+		if (second && wakeup_preempt_entity(second, left) < 1)
+			se = second;
+	}
+
+	if (cfs_rq->next && wakeup_preempt_entity(cfs_rq->next, left) < 1) {
+		/*
+		 * Someone really wants this to run. If it's not unfair, run it.
+		 */
+		se = cfs_rq->next;
+	} else if (cfs_rq->last && wakeup_preempt_entity(cfs_rq->last, left) < 1) {
+		/*
+		 * Prefer last buddy, try to return the CPU to a preempted task.
+		 */
+		se = cfs_rq->last;
+	}
+
+	return se;
+}
+#elif SCHED_VERSION == 1
+static struct sched_entity *
+pick_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *curr)
+// clang-format on
+{
+	struct sched_entity *left = __pick_first_entity(cfs_rq);
+	struct sched_entity *se;
+	stats_add(&total_calls, 1);
+	/*
+	 * If curr is set we have to see if its left of the leftmost entity
+	 * still in the tree, provided there was anything in the tree at all.
+	 */
+	if (!left || (curr && entity_before(curr, left)))
+		left = curr;
+
+	se = left; /* ideally we run the leftmost entity */
+
 	/*
 	 * Avoid running the skip buddy, if running something else can
 	 * be done without getting too unfair.
@@ -4629,9 +4687,489 @@ pick_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *curr)
 		 */
 		se = cfs_rq->last;
 	}
+	stats_add(&out_dont_sched, se && auto_lock_dont_sched(task_of(se)));
+
+	stats_print(sched_print_throttle,
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n",
+		    STATS_V_OUT(total_calls), STATS_V_OUT(out_dont_sched));
+	return se;
+}
+#elif SCHED_VERSION == 2
+static struct sched_entity *pick_next_entity(struct cfs_rq *cfs_rq,
+					     struct sched_entity *curr)
+{
+	struct sched_entity *left = __pick_first_entity(cfs_rq);
+	struct sched_entity *se;
+
+	stats_add(&total_calls, 1);
+	stats_add(&curr_dont_sched,
+		  curr && auto_lock_dont_sched(task_of(curr)));
+	stats_add(&left_dont_sched,
+		  left && auto_lock_dont_sched(task_of(left)));
+	/*
+	 * If curr is set we have to see if its left of the leftmost entity
+	 * still in the tree, provided there was anything in the tree at all.
+	 */
+	if (!left || (curr && entity_before(curr, left))) {
+		stats_add(&replace_left_curr_dont_sched,
+			  curr && auto_lock_dont_sched(task_of(curr)));
+		stats_add(&replace_left_left_dont_sched,
+			  left && auto_lock_dont_sched(task_of(left)));
+		stats_add(&replace_left, 1);
+		left = curr;
+	}
+
+	se = left; /* ideally we run the leftmost entity */
+
+	/*
+	 * Avoid running the skip buddy, if running something else can
+	 * be done without getting too unfair.
+	 */
+	if (cfs_rq->skip && cfs_rq->skip == se) {
+		struct sched_entity *second;
+		stats_add(&se_skip, 1);
+		if (se == curr) {
+			stats_add(&se_eq_curr, 1);
+			second = __pick_first_entity(cfs_rq);
+			stats_add(&sec_second_dont_sched,
+				  second && auto_lock_dont_sched(
+						    task_of(second)));
+		} else {
+			stats_add(&se_neq_curr, 1);
+			second = __pick_next_entity(se);
+			if (!second || (curr && entity_before(curr, second))) {
+				stats_add(&second_eq_curr, 1);
+				second = curr;
+			}
+
+			stats_add(&snc_second_dont_sched,
+				  second && auto_lock_dont_sched(
+						    task_of(second)));
+		}
+
+		if (second && wakeup_preempt_entity(second, left) < 1) {
+			stats_add(&replace_wakeup_second, 1);
+			se = second;
+		}
+	}
+
+	if (cfs_rq->next && wakeup_preempt_entity(cfs_rq->next, left) < 1) {
+		/*
+		 * Someone really wants this to run. If it's not unfair, run it.
+		 */
+		stats_add(&replace_wakeup_next, 1);
+		stats_add(&next_dont_sched,
+			  auto_lock_dont_sched(task_of(cfs_rq->next)));
+		se = cfs_rq->next;
+	} else if (cfs_rq->last &&
+		   wakeup_preempt_entity(cfs_rq->last, left) < 1) {
+		stats_add(&replace_wakeup_last, 1);
+		/*
+		 * Prefer last buddy, try to return the CPU to a preempted task.
+		 */
+		stats_add(&replace_wakeup_last, 1);
+		stats_add(&next_dont_sched,
+			  auto_lock_dont_sched(task_of(cfs_rq->last)));
+		se = cfs_rq->last;
+	}
+	stats_add(&out_dont_sched, se && auto_lock_dont_sched(task_of(se)));
+	stats_add(&out_may_sched, se && !auto_lock_dont_sched(task_of(se)));
+
+	stats_add(&out_has_al_ctx, se && auto_lock_exists(task_of(se)));
+	stats_add(&out_has_al, se && auto_lock_has_al(task_of(se)));
+	stats_add(&out_is_armed, se && auto_lock_is_armed_s(task_of(se)));
+
+	stats_add(&out_is_null, !se);
+
+	stats_print(sched_print_throttle,
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n",
+		    STATS_V_OUT(total_calls), STATS_V_OUT(out_dont_sched),
+		    STATS_V_OUT(out_may_sched), STATS_V_OUT(out_is_null),
+		    STATS_V_OUT(out_has_al_ctx), STATS_V_OUT(out_has_al),
+		    STATS_V_OUT(out_is_armed), STATS_V_OUT(curr_dont_sched),
+		    STATS_V_OUT(left_dont_sched), STATS_V_OUT(se_dont_sched),
+		    STATS_V_OUT(next_dont_sched), STATS_V_OUT(last_dont_sched),
+		    STATS_V_OUT(replace_left_left_dont_sched),
+		    STATS_V_OUT(replace_left_curr_dont_sched),
+		    STATS_V_OUT(se_skip), STATS_V_OUT(se_eq_curr),
+		    STATS_V_OUT(se_neq_curr), STATS_V_OUT(second_eq_curr),
+		    STATS_V_OUT(sec_second_dont_sched),
+		    STATS_V_OUT(snc_second_dont_sched),
+		    STATS_V_OUT(replace_left),
+		    STATS_V_OUT(replace_wakeup_second),
+		    STATS_V_OUT(replace_wakeup_next),
+		    STATS_V_OUT(replace_wakeup_last));
+
+	return se;
+}
+#elif SCHED_VERSION == 3
+static struct sched_entity *pick_next_entity(struct cfs_rq *cfs_rq,
+					     struct sched_entity *curr)
+{
+	struct sched_entity *left = __pick_first_entity(cfs_rq);
+	struct sched_entity *se;
+
+	stats_add(&total_calls, 1);
+	stats_add(&curr_dont_sched,
+		  curr && auto_lock_dont_sched(task_of(curr)));
+	stats_add(&left_dont_sched,
+		  left && auto_lock_dont_sched(task_of(left)));
+	/*
+	 * If curr is set we have to see if its left of the leftmost entity
+	 * still in the tree, provided there was anything in the tree at all.
+	 */
+	if (!left || (curr && entity_before(curr, left))) {
+		stats_add(&replace_left_curr_dont_sched,
+			  curr && auto_lock_dont_sched(task_of(curr)));
+		stats_add(&replace_left_left_dont_sched,
+			  left && auto_lock_dont_sched(task_of(left)));
+		stats_add(&replace_left, 1);
+		left = curr;
+	}
+
+	se = left; /* ideally we run the leftmost entity */
+
+	if (se) {
+		struct sched_entity *second;
+		if (auto_lock_dont_sched(task_of(se))) {
+			if (se == curr) {
+				second = __pick_first_entity(cfs_rq);
+				if (!second) {
+					goto pick_next_from_curr;
+				}
+				if (auto_lock_dont_sched(task_of(second))) {
+					second = __pick_next_entity(se);
+					goto pick_next_loop;
+				}
+				se = second;
+				goto return_good;
+			} else {
+				second = __pick_next_entity(se);
+				if (second == NULL) {
+				pick_next_from_curr:
+					second = curr;
+				}
+			pick_next_loop:
+				while (second) {
+					if (!auto_lock_dont_sched(
+						    task_of(second))) {
+						se = second;
+						goto return_good;
+					}
+					second = __pick_next_entity(second);
+				}
+			}
+
+		} else if (cfs_rq->skip == se) {
+			/*
+             * Avoid running the skip buddy, if running something else can
+             * be done without getting too unfair.
+             */
+			stats_add(&se_skip, 1);
+			if (se == curr) {
+				stats_add(&se_eq_curr, 1);
+				second = __pick_first_entity(cfs_rq);
+				stats_add(&sec_second_dont_sched,
+					  second && auto_lock_dont_sched(
+							    task_of(second)));
+			} else {
+				stats_add(&se_neq_curr, 1);
+				second = __pick_next_entity(se);
+				if (!second ||
+				    (curr && entity_before(curr, second))) {
+					stats_add(&second_eq_curr, 1);
+					second = curr;
+				}
+
+				stats_add(&snc_second_dont_sched,
+					  second && auto_lock_dont_sched(
+							    task_of(second)));
+			}
+
+			if (second && !auto_lock_dont_sched(task_of(second)) &&
+			    wakeup_preempt_entity(second, left) < 1) {
+				stats_add(&replace_wakeup_second, 1);
+				se = second;
+				goto return_good;
+			}
+		}
+	}
+
+	if (cfs_rq->next && wakeup_preempt_entity(cfs_rq->next, left) < 1) {
+		/*
+		 * Someone really wants this to run. If it's not unfair, run it.
+		 */
+		stats_add(&replace_wakeup_next, 1);
+		stats_add(&next_dont_sched,
+			  auto_lock_dont_sched(task_of(cfs_rq->next)));
+		se = cfs_rq->next;
+	} else if (cfs_rq->last &&
+		   wakeup_preempt_entity(cfs_rq->last, left) < 1) {
+		stats_add(&replace_wakeup_last, 1);
+		/*
+		 * Prefer last buddy, try to return the CPU to a preempted task.
+		 */
+		stats_add(&replace_wakeup_last, 1);
+		stats_add(&next_dont_sched,
+			  auto_lock_dont_sched(task_of(cfs_rq->last)));
+		se = cfs_rq->last;
+	}
+
+return_good:
+	stats_add(&out_dont_sched, se && auto_lock_dont_sched(task_of(se)));
+	stats_add(&out_may_sched, se && !auto_lock_dont_sched(task_of(se)));
+
+	stats_add(&out_has_al_ctx, se && auto_lock_exists(task_of(se)));
+	stats_add(&out_has_al, se && auto_lock_has_al(task_of(se)));
+	stats_add(&out_is_armed, se && auto_lock_is_armed_s(task_of(se)));
+
+	stats_add(&out_is_null, !se);
+
+	stats_print(sched_print_throttle,
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n",
+		    STATS_V_OUT(total_calls), STATS_V_OUT(out_dont_sched),
+		    STATS_V_OUT(out_may_sched), STATS_V_OUT(out_is_null),
+		    STATS_V_OUT(out_has_al_ctx), STATS_V_OUT(out_has_al),
+		    STATS_V_OUT(out_is_armed), STATS_V_OUT(curr_dont_sched),
+		    STATS_V_OUT(left_dont_sched), STATS_V_OUT(se_dont_sched),
+		    STATS_V_OUT(next_dont_sched), STATS_V_OUT(last_dont_sched),
+		    STATS_V_OUT(replace_left_left_dont_sched),
+		    STATS_V_OUT(replace_left_curr_dont_sched),
+		    STATS_V_OUT(se_skip), STATS_V_OUT(se_eq_curr),
+		    STATS_V_OUT(se_neq_curr), STATS_V_OUT(second_eq_curr),
+		    STATS_V_OUT(sec_second_dont_sched),
+		    STATS_V_OUT(snc_second_dont_sched),
+		    STATS_V_OUT(replace_left),
+		    STATS_V_OUT(replace_wakeup_second),
+		    STATS_V_OUT(replace_wakeup_next),
+		    STATS_V_OUT(replace_wakeup_last));
+
+	return se;
+}
+#elif SCHED_VERSION == 4
+static struct sched_entity *pick_next_entity(struct cfs_rq *cfs_rq,
+					     struct sched_entity *curr)
+// clang-format on
+{
+	struct sched_entity *left = __pick_first_entity(cfs_rq);
+	struct sched_entity *se;
+	int dont_sched_se = 0;
+	stats_add(&total_calls, 1);
+	/*
+	 * If curr is set we have to see if its left of the leftmost entity
+	 * still in the tree, provided there was anything in the tree at all.
+	 */
+	if (!left || (curr && entity_before(curr, left)))
+		left = curr;
+
+	se = left; /* ideally we run the leftmost entity */
+
+	/*
+	 * Avoid running the skip buddy, if running something else can
+	 * be done without getting too unfair.
+	 */
+	if (se && (cfs_rq->skip == se ||
+		   (dont_sched_se = auto_lock_dont_sched(task_of(se))))) {
+		struct sched_entity *second;
+
+		if (se == curr && !dont_sched_se) {
+			LPRINTK_VVVV("se == curr\n");
+			second = __pick_first_entity(cfs_rq);
+		} else {
+			LPRINTK_VVVV("se != curr\n");
+			second = __pick_next_entity(se);
+			if (!second || (curr && entity_before(curr, second))) {
+				LPRINTK_VVVV("second = curr\n");
+				second = curr;
+			} else {
+				struct sched_entity *next_second = second;
+				for (;;) {
+					if (!auto_lock_dont_sched(
+						    task_of(next_second))) {
+						second = next_second;
+						goto try_replace;
+					}
+					next_second =
+						__pick_next_entity(next_second);
+					if (!next_second) {
+						goto failed_finding_better;
+					}
+				}
+			}
+		}
+
+		if (second) {
+		try_replace:
+			if (dont_sched_se) {
+				LPRINTK_VVVV("Setting se = second (A)\n");
+				se = second;
+			} else {
+			failed_finding_better:
+				if ((!auto_lock_dont_sched(task_of(second))) &&
+				    wakeup_preempt_entity(second, left) < 1) {
+					LPRINTK_VVVV(
+						"Setting se = second (B)\n");
+					se = second;
+				}
+			}
+		}
+	}
+
+	if (cfs_rq->next && wakeup_preempt_entity(cfs_rq->next, left) < 1) {
+		LPRINTK_VVVV("Taking next\n");
+		/*
+		 * Someone really wants this to run. If it's not unfair, run it.
+		 */
+		se = cfs_rq->next;
+	} else if (cfs_rq->last &&
+		   wakeup_preempt_entity(cfs_rq->last, left) < 1) {
+		LPRINTK_VVVV("Taking Last\n");
+		/*
+		 * Prefer last buddy, try to return the CPU to a preempted task.
+		 */
+		se = cfs_rq->last;
+	}
+	stats_add(&out_dont_sched, se && auto_lock_dont_sched(task_of(se)));
+
+	stats_print(sched_print_throttle,
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n",
+		    STATS_V_OUT(total_calls), STATS_V_OUT(out_dont_sched));
+	return se;
+}
+#else
+static struct sched_entity *pick_next_entity(struct cfs_rq *cfs_rq,
+					     struct sched_entity *curr)
+// clang-format on
+{
+	struct sched_entity *left = __pick_first_entity(cfs_rq);
+	struct sched_entity *se;
+	int dont_sched_se = 0;
+	stats_add(&total_calls, 1);
+	/*
+	 * If curr is set we have to see if its left of the leftmost entity
+	 * still in the tree, provided there was anything in the tree at all.
+	 */
+	if (!left || (curr && entity_before(curr, left)))
+		left = curr;
+
+	se = left; /* ideally we run the leftmost entity */
+
+	/*
+	 * Avoid running the skip buddy, if running something else can
+	 * be done without getting too unfair.
+	 */
+	if (se && (cfs_rq->skip == se ||
+		   (dont_sched_se = auto_lock_dont_sched(task_of(se))))) {
+		struct sched_entity *second;
+
+		if (se == curr) {
+			LPRINTK_VVVV("se == curr\n");
+			second = __pick_first_entity(cfs_rq);
+		} else {
+			LPRINTK_VVVV("se != curr\n");
+			second = __pick_next_entity(se);
+			if (!second || (curr && entity_before(curr, second))) {
+				LPRINTK_VVVV("second = curr\n");
+				second = curr;
+			} else if (dont_sched_se) {
+				struct sched_entity *next_second = second;
+				for (;;) {
+					if (!auto_lock_dont_sched(
+						    task_of(next_second))) {
+						return next_second;
+					}
+					next_second =
+						__pick_next_entity(next_second);
+					if (!next_second) {
+						break;
+					}
+				}
+			}
+		}
+
+		if (second) {
+			if (dont_sched_se) {
+				LPRINTK_VVVV("Setting se = second (A)\n");
+				se = second;
+			} else if ((!auto_lock_dont_sched(task_of(second))) &&
+				   wakeup_preempt_entity(second, left) < 1) {
+				LPRINTK_VVVV("Setting se = second (B)\n");
+				se = second;
+			}
+		}
+	}
+
+	if (cfs_rq->next && wakeup_preempt_entity(cfs_rq->next, left) < 1) {
+		LPRINTK_VVVV("Taking next\n");
+		/*
+		 * Someone really wants this to run. If it's not unfair, run it.
+		 */
+		se = cfs_rq->next;
+	} else if (cfs_rq->last &&
+		   wakeup_preempt_entity(cfs_rq->last, left) < 1) {
+		LPRINTK_VVVV("Taking Last\n");
+		/*
+		 * Prefer last buddy, try to return the CPU to a preempted task.
+		 */
+		se = cfs_rq->last;
+	}
+	stats_add(&out_dont_sched, se && auto_lock_dont_sched(task_of(se)));
 
+	stats_print(sched_print_throttle,
+		    "%-24s: %10u\n"
+		    "%-24s: %10u\n",
+		    STATS_V_OUT(total_calls), STATS_V_OUT(out_dont_sched));
 	return se;
 }
+#endif
 //clang-format off
 static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq);
 
@@ -4659,8 +5197,8 @@ static void put_prev_entity(struct cfs_rq *cfs_rq, struct sched_entity *prev)
 	cfs_rq->curr = NULL;
 }
 
-static void
-entity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)
+static void entity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr,
+			int queued)
 {
 	/*
 	 * Update run-time statistics of the 'current'.
@@ -4686,7 +5224,7 @@ entity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)
 	 * don't let the period tick interfere with the hrtick preemption
 	 */
 	if (!sched_feat(DOUBLE_TICK) &&
-			hrtimer_active(&rq_of(cfs_rq)->hrtick_timer))
+	    hrtimer_active(&rq_of(cfs_rq)->hrtick_timer))
 		return;
 #endif
 
@@ -4694,7 +5232,6 @@ entity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)
 		check_preempt_tick(cfs_rq, curr);
 }
 
-
 /**************************************************
  * CFS bandwidth control machinery
  */
@@ -4724,8 +5261,12 @@ static bool cfs_bandwidth_used(void)
 	return true;
 }
 
-void cfs_bandwidth_usage_inc(void) {}
-void cfs_bandwidth_usage_dec(void) {}
+void cfs_bandwidth_usage_inc(void)
+{
+}
+void cfs_bandwidth_usage_dec(void)
+{
+}
 #endif /* CONFIG_JUMP_LABEL */
 
 /*
@@ -4807,7 +5348,8 @@ static int assign_cfs_rq_runtime(struct cfs_rq *cfs_rq)
 	int ret;
 
 	raw_spin_lock(&cfs_b->lock);
-	ret = __assign_cfs_rq_runtime(cfs_b, cfs_rq, sched_cfs_bandwidth_slice());
+	ret = __assign_cfs_rq_runtime(cfs_b, cfs_rq,
+				      sched_cfs_bandwidth_slice());
 	raw_spin_unlock(&cfs_b->lock);
 
 	return ret;
@@ -4831,8 +5373,8 @@ static void __account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)
 		resched_curr(rq_of(cfs_rq));
 }
 
-static __always_inline
-void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)
+static __always_inline void account_cfs_rq_runtime(struct cfs_rq *cfs_rq,
+						   u64 delta_exec)
 {
 	if (!cfs_bandwidth_used() || !cfs_rq->runtime_enabled)
 		return;
@@ -4856,8 +5398,8 @@ static inline int throttled_hierarchy(struct cfs_rq *cfs_rq)
  * dest_cpu are members of a throttled hierarchy when performing group
  * load-balance operations.
  */
-static inline int throttled_lb_pair(struct task_group *tg,
-				    int src_cpu, int dest_cpu)
+static inline int throttled_lb_pair(struct task_group *tg, int src_cpu,
+				    int dest_cpu)
 {
 	struct cfs_rq *src_cfs_rq, *dest_cfs_rq;
 
@@ -4875,8 +5417,8 @@ static int tg_unthrottle_up(struct task_group *tg, void *data)
 
 	cfs_rq->throttle_count--;
 	if (!cfs_rq->throttle_count) {
-		cfs_rq->throttled_clock_task_time += rq_clock_task(rq) -
-					     cfs_rq->throttled_clock_task;
+		cfs_rq->throttled_clock_task_time +=
+			rq_clock_task(rq) - cfs_rq->throttled_clock_task;
 
 		/* Add cfs_rq with load or one or more already running entities to the list */
 		if (!cfs_rq_is_decayed(cfs_rq) || cfs_rq->nr_running)
@@ -4927,7 +5469,7 @@ static bool throttle_cfs_rq(struct cfs_rq *cfs_rq)
 	raw_spin_unlock(&cfs_b->lock);
 
 	if (!dequeue)
-		return false;  /* Throttle no longer required. */
+		return false; /* Throttle no longer required. */
 
 	se = cfs_rq->tg->se[cpu_of(rq_of(cfs_rq))];
 
@@ -4938,7 +5480,8 @@ static bool throttle_cfs_rq(struct cfs_rq *cfs_rq)
 
 	task_delta = cfs_rq->h_nr_running;
 	idle_task_delta = cfs_rq->idle_h_nr_running;
-	for_each_sched_entity(se) {
+	for_each_sched_entity(se)
+	{
 		struct cfs_rq *qcfs_rq = cfs_rq_of(se);
 		/* throttled entity or throttle-on-deactivate */
 		if (!se->on_rq)
@@ -4959,7 +5502,8 @@ static bool throttle_cfs_rq(struct cfs_rq *cfs_rq)
 		}
 	}
 
-	for_each_sched_entity(se) {
+	for_each_sched_entity(se)
+	{
 		struct cfs_rq *qcfs_rq = cfs_rq_of(se);
 		/* throttled entity or throttle-on-deactivate */
 		if (!se->on_rq)
@@ -5018,7 +5562,8 @@ void unthrottle_cfs_rq(struct cfs_rq *cfs_rq)
 
 	task_delta = cfs_rq->h_nr_running;
 	idle_task_delta = cfs_rq->idle_h_nr_running;
-	for_each_sched_entity(se) {
+	for_each_sched_entity(se)
+	{
 		struct cfs_rq *qcfs_rq = cfs_rq_of(se);
 
 		if (se->on_rq)
@@ -5036,7 +5581,8 @@ void unthrottle_cfs_rq(struct cfs_rq *cfs_rq)
 			goto unthrottle_throttle;
 	}
 
-	for_each_sched_entity(se) {
+	for_each_sched_entity(se)
+	{
 		struct cfs_rq *qcfs_rq = cfs_rq_of(se);
 
 		update_load_avg(qcfs_rq, se, UPDATE_TG);
@@ -5069,7 +5615,8 @@ void unthrottle_cfs_rq(struct cfs_rq *cfs_rq)
 	 * incomplete leaf list maintenance, resulting in triggering the
 	 * assertion below.
 	 */
-	for_each_sched_entity(se) {
+	for_each_sched_entity(se)
+	{
 		struct cfs_rq *qcfs_rq = cfs_rq_of(se);
 
 		if (list_add_leaf_cfs_rq(qcfs_rq))
@@ -5089,8 +5636,8 @@ static void distribute_cfs_runtime(struct cfs_bandwidth *cfs_b)
 	u64 runtime, remaining = 1;
 
 	rcu_read_lock();
-	list_for_each_entry_rcu(cfs_rq, &cfs_b->throttled_cfs_rq,
-				throttled_list) {
+	list_for_each_entry_rcu (cfs_rq, &cfs_b->throttled_cfs_rq,
+				 throttled_list) {
 		struct rq *rq = rq_of(cfs_rq);
 		struct rq_flags rf;
 
@@ -5115,7 +5662,7 @@ static void distribute_cfs_runtime(struct cfs_bandwidth *cfs_b)
 		if (cfs_rq->runtime_remaining > 0)
 			unthrottle_cfs_rq(cfs_rq);
 
-next:
+	next:
 		rq_unlock_irqrestore(rq, &rf);
 
 		if (!remaining)
@@ -5130,7 +5677,8 @@ static void distribute_cfs_runtime(struct cfs_bandwidth *cfs_b)
  * period the timer is deactivated until scheduling resumes; cfs_b->idle is
  * used to track this state.
  */
-static int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun, unsigned long flags)
+static int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun,
+				     unsigned long flags)
 {
 	int throttled;
 
@@ -5231,8 +5779,8 @@ static void start_cfs_slack_bandwidth(struct cfs_bandwidth *cfs_b)
 	cfs_b->slack_started = true;
 
 	hrtimer_start(&cfs_b->slack_timer,
-			ns_to_ktime(cfs_bandwidth_slack_period),
-			HRTIMER_MODE_REL);
+		      ns_to_ktime(cfs_bandwidth_slack_period),
+		      HRTIMER_MODE_REL);
 }
 
 /* we know any runtime found here is valid as update_curr() precedes return */
@@ -5403,13 +5951,13 @@ static enum hrtimer_restart sched_cfs_period_timer(struct hrtimer *timer)
 				cfs_b->burst *= 2;
 
 				pr_warn_ratelimited(
-	"cfs_period_timer[cpu%d]: period too short, scaling up (new cfs_period_us = %lld, cfs_quota_us = %lld)\n",
+					"cfs_period_timer[cpu%d]: period too short, scaling up (new cfs_period_us = %lld, cfs_quota_us = %lld)\n",
 					smp_processor_id(),
 					div_u64(new, NSEC_PER_USEC),
 					div_u64(cfs_b->quota, NSEC_PER_USEC));
 			} else {
 				pr_warn_ratelimited(
-	"cfs_period_timer[cpu%d]: period too short, but cannot scale up without losing precision (cfs_period_us = %lld, cfs_quota_us = %lld)\n",
+					"cfs_period_timer[cpu%d]: period too short, but cannot scale up without losing precision (cfs_period_us = %lld, cfs_quota_us = %lld)\n",
 					smp_processor_id(),
 					div_u64(old, NSEC_PER_USEC),
 					div_u64(cfs_b->quota, NSEC_PER_USEC));
@@ -5435,7 +5983,8 @@ void init_cfs_bandwidth(struct cfs_bandwidth *cfs_b)
 	cfs_b->burst = 0;
 
 	INIT_LIST_HEAD(&cfs_b->throttled_cfs_rq);
-	hrtimer_init(&cfs_b->period_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED);
+	hrtimer_init(&cfs_b->period_timer, CLOCK_MONOTONIC,
+		     HRTIMER_MODE_ABS_PINNED);
 	cfs_b->period_timer.function = sched_cfs_period_timer;
 	hrtimer_init(&cfs_b->slack_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	cfs_b->slack_timer.function = sched_cfs_slack_timer;
@@ -5485,7 +6034,7 @@ static void __maybe_unused update_runtime_enabled(struct rq *rq)
 	lockdep_assert_rq_held(rq);
 
 	rcu_read_lock();
-	list_for_each_entry_rcu(tg, &task_groups, list) {
+	list_for_each_entry_rcu (tg, &task_groups, list) {
 		struct cfs_bandwidth *cfs_b = &tg->cfs_bandwidth;
 		struct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];
 
@@ -5504,7 +6053,7 @@ static void __maybe_unused unthrottle_offline_cfs_rqs(struct rq *rq)
 	lockdep_assert_rq_held(rq);
 
 	rcu_read_lock();
-	list_for_each_entry_rcu(tg, &task_groups, list) {
+	list_for_each_entry_rcu (tg, &task_groups, list) {
 		struct cfs_rq *cfs_rq = tg->cfs_rq[cpu_of(rq)];
 
 		if (!cfs_rq->runtime_enabled)
@@ -5534,11 +6083,22 @@ static inline bool cfs_bandwidth_used(void)
 	return false;
 }
 
-static void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec) {}
-static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq) { return false; }
-static void check_enqueue_throttle(struct cfs_rq *cfs_rq) {}
-static inline void sync_throttle(struct task_group *tg, int cpu) {}
-static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq) {}
+static void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)
+{
+}
+static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq)
+{
+	return false;
+}
+static void check_enqueue_throttle(struct cfs_rq *cfs_rq)
+{
+}
+static inline void sync_throttle(struct task_group *tg, int cpu)
+{
+}
+static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq)
+{
+}
 
 static inline int cfs_rq_throttled(struct cfs_rq *cfs_rq)
 {
@@ -5550,25 +6110,35 @@ static inline int throttled_hierarchy(struct cfs_rq *cfs_rq)
 	return 0;
 }
 
-static inline int throttled_lb_pair(struct task_group *tg,
-				    int src_cpu, int dest_cpu)
+static inline int throttled_lb_pair(struct task_group *tg, int src_cpu,
+				    int dest_cpu)
 {
 	return 0;
 }
 
-void init_cfs_bandwidth(struct cfs_bandwidth *cfs_b) {}
+void init_cfs_bandwidth(struct cfs_bandwidth *cfs_b)
+{
+}
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
-static void init_cfs_rq_runtime(struct cfs_rq *cfs_rq) {}
+static void init_cfs_rq_runtime(struct cfs_rq *cfs_rq)
+{
+}
 #endif
 
 static inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)
 {
 	return NULL;
 }
-static inline void destroy_cfs_bandwidth(struct cfs_bandwidth *cfs_b) {}
-static inline void update_runtime_enabled(struct rq *rq) {}
-static inline void unthrottle_offline_cfs_rqs(struct rq *rq) {}
+static inline void destroy_cfs_bandwidth(struct cfs_bandwidth *cfs_b)
+{
+}
+static inline void update_runtime_enabled(struct rq *rq)
+{
+}
+static inline void unthrottle_offline_cfs_rqs(struct rq *rq)
+{
+}
 
 #endif /* CONFIG_CFS_BANDWIDTH */
 
@@ -5614,8 +6184,7 @@ static void hrtick_update(struct rq *rq)
 		hrtick_start_fair(rq, curr);
 }
 #else /* !CONFIG_SCHED_HRTICK */
-static inline void
-hrtick_start_fair(struct rq *rq, struct task_struct *p)
+static inline void hrtick_start_fair(struct rq *rq, struct task_struct *p)
 {
 }
 
@@ -5638,7 +6207,9 @@ static inline void update_overutilized_status(struct rq *rq)
 	}
 }
 #else
-static inline void update_overutilized_status(struct rq *rq) { }
+static inline void update_overutilized_status(struct rq *rq)
+{
+}
 #endif
 
 /* Runqueue only has SCHED_IDLE tasks enqueued */
@@ -5656,7 +6227,7 @@ static int sched_idle_rq(struct rq *rq)
 static bool sched_idle_cfs_rq(struct cfs_rq *cfs_rq)
 {
 	return cfs_rq->nr_running &&
-		cfs_rq->nr_running == cfs_rq->idle_nr_running;
+	       cfs_rq->nr_running == cfs_rq->idle_nr_running;
 }
 
 #ifdef CONFIG_SMP
@@ -5671,8 +6242,7 @@ static int sched_idle_cpu(int cpu)
  * increased. Here we update the fair scheduling stats and
  * then put the task into the rbtree:
  */
-static void
-enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
+static void enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 {
 	struct cfs_rq *cfs_rq;
 	struct sched_entity *se = &p->se;
@@ -5695,7 +6265,8 @@ enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 	if (p->in_iowait)
 		cpufreq_update_util(rq, SCHED_CPUFREQ_IOWAIT);
 
-	for_each_sched_entity(se) {
+	for_each_sched_entity(se)
+	{
 		if (se->on_rq)
 			break;
 		cfs_rq = cfs_rq_of(se);
@@ -5714,7 +6285,8 @@ enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 		flags = ENQUEUE_WAKEUP;
 	}
 
-	for_each_sched_entity(se) {
+	for_each_sched_entity(se)
+	{
 		cfs_rq = cfs_rq_of(se);
 
 		update_load_avg(cfs_rq, se, UPDATE_TG);
@@ -5731,12 +6303,12 @@ enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 		if (cfs_rq_throttled(cfs_rq))
 			goto enqueue_throttle;
 
-               /*
+		/*
                 * One parent has been throttled and cfs_rq removed from the
                 * list. Add it back to not break the leaf list.
                 */
-               if (throttled_hierarchy(cfs_rq))
-                       list_add_leaf_cfs_rq(cfs_rq);
+		if (throttled_hierarchy(cfs_rq))
+			list_add_leaf_cfs_rq(cfs_rq);
 	}
 
 	/* At this point se is NULL and we are at root level*/
@@ -5767,7 +6339,8 @@ enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 		 * leaf list maintenance, resulting in triggering the assertion
 		 * below.
 		 */
-		for_each_sched_entity(se) {
+		for_each_sched_entity(se)
+		{
 			cfs_rq = cfs_rq_of(se);
 
 			if (list_add_leaf_cfs_rq(cfs_rq))
@@ -5797,7 +6370,8 @@ static void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 
 	util_est_dequeue(&rq->cfs, p);
 
-	for_each_sched_entity(se) {
+	for_each_sched_entity(se)
+	{
 		cfs_rq = cfs_rq_of(se);
 		dequeue_entity(cfs_rq, se, flags);
 
@@ -5826,7 +6400,8 @@ static void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 		flags |= DEQUEUE_SLEEP;
 	}
 
-	for_each_sched_entity(se) {
+	for_each_sched_entity(se)
+	{
 		cfs_rq = cfs_rq_of(se);
 
 		update_load_avg(cfs_rq, se, UPDATE_TG);
@@ -5842,7 +6417,6 @@ static void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 		/* end evaluation on encountering a throttled cfs_rq */
 		if (cfs_rq_throttled(cfs_rq))
 			goto dequeue_throttle;
-
 	}
 
 	/* At this point se is NULL and we are at root level*/
@@ -5868,10 +6442,10 @@ DEFINE_PER_CPU(cpumask_var_t, select_idle_mask);
 static struct {
 	cpumask_var_t idle_cpus_mask;
 	atomic_t nr_cpus;
-	int has_blocked;		/* Idle CPUS has blocked load */
-	int needs_update;		/* Newly idle CPUs need their next_balance collated */
-	unsigned long next_balance;     /* in jiffy units */
-	unsigned long next_blocked;	/* Next update of blocked load in jiffies */
+	int has_blocked; /* Idle CPUS has blocked load */
+	int needs_update; /* Newly idle CPUs need their next_balance collated */
+	unsigned long next_balance; /* in jiffy units */
+	unsigned long next_blocked; /* Next update of blocked load in jiffies */
 } nohz ____cacheline_aligned;
 
 #endif /* CONFIG_NO_HZ_COMMON */
@@ -5999,8 +6573,7 @@ static int wake_wide(struct task_struct *p)
  *			  scheduling latency of the CPUs. This seems to work
  *			  for the overloaded case.
  */
-static int
-wake_affine_idle(int this_cpu, int prev_cpu, int sync)
+static int wake_affine_idle(int this_cpu, int prev_cpu, int sync)
 {
 	/*
 	 * If this_cpu is idle, it implies the wakeup is from interrupt
@@ -6014,7 +6587,8 @@ wake_affine_idle(int this_cpu, int prev_cpu, int sync)
 	 * a cpufreq perspective, it's better to have higher utilisation
 	 * on one CPU.
 	 */
-	if (available_idle_cpu(this_cpu) && cpus_share_cache(this_cpu, prev_cpu))
+	if (available_idle_cpu(this_cpu) &&
+	    cpus_share_cache(this_cpu, prev_cpu))
 		return available_idle_cpu(prev_cpu) ? prev_cpu : this_cpu;
 
 	if (sync && cpu_rq(this_cpu)->nr_running == 1)
@@ -6026,9 +6600,8 @@ wake_affine_idle(int this_cpu, int prev_cpu, int sync)
 	return nr_cpumask_bits;
 }
 
-static int
-wake_affine_weight(struct sched_domain *sd, struct task_struct *p,
-		   int this_cpu, int prev_cpu, int sync)
+static int wake_affine_weight(struct sched_domain *sd, struct task_struct *p,
+			      int this_cpu, int prev_cpu, int sync)
 {
 	s64 this_eff_load, prev_eff_load;
 	unsigned long task_load;
@@ -6095,8 +6668,8 @@ find_idlest_group(struct sched_domain *sd, struct task_struct *p, int this_cpu);
 /*
  * find_idlest_group_cpu - find the idlest CPU among the CPUs in the group.
  */
-static int
-find_idlest_group_cpu(struct sched_group *group, struct task_struct *p, int this_cpu)
+static int find_idlest_group_cpu(struct sched_group *group,
+				 struct task_struct *p, int this_cpu)
 {
 	unsigned long load, min_load = ULONG_MAX;
 	unsigned int min_exit_latency = UINT_MAX;
@@ -6110,7 +6683,7 @@ find_idlest_group_cpu(struct sched_group *group, struct task_struct *p, int this
 		return cpumask_first(sched_group_span(group));
 
 	/* Traverse only the allowed CPUs */
-	for_each_cpu_and(i, sched_group_span(group), p->cpus_ptr) {
+	for_each_cpu_and (i, sched_group_span(group), p->cpus_ptr) {
 		struct rq *rq = cpu_rq(i);
 
 		if (!sched_core_cookie_match(rq, p))
@@ -6130,7 +6703,8 @@ find_idlest_group_cpu(struct sched_group *group, struct task_struct *p, int this
 				min_exit_latency = idle->exit_latency;
 				latest_idle_timestamp = rq->idle_stamp;
 				shallowest_idle_cpu = i;
-			} else if ((!idle || idle->exit_latency == min_exit_latency) &&
+			} else if ((!idle ||
+				    idle->exit_latency == min_exit_latency) &&
 				   rq->idle_stamp > latest_idle_timestamp) {
 				/*
 				 * If equal or no active idle state, then
@@ -6149,11 +6723,13 @@ find_idlest_group_cpu(struct sched_group *group, struct task_struct *p, int this
 		}
 	}
 
-	return shallowest_idle_cpu != -1 ? shallowest_idle_cpu : least_loaded_cpu;
+	return shallowest_idle_cpu != -1 ? shallowest_idle_cpu :
+					   least_loaded_cpu;
 }
 
-static inline int find_idlest_cpu(struct sched_domain *sd, struct task_struct *p,
-				  int cpu, int prev_cpu, int sd_flag)
+static inline int find_idlest_cpu(struct sched_domain *sd,
+				  struct task_struct *p, int cpu, int prev_cpu,
+				  int sd_flag)
 {
 	int new_cpu = cpu;
 
@@ -6194,7 +6770,8 @@ static inline int find_idlest_cpu(struct sched_domain *sd, struct task_struct *p
 		cpu = new_cpu;
 		weight = sd->span_weight;
 		sd = NULL;
-		for_each_domain(cpu, tmp) {
+		for_each_domain(cpu, tmp)
+		{
 			if (weight <= tmp->span_weight)
 				break;
 			if (tmp->flags & sd_flag)
@@ -6254,7 +6831,7 @@ void __update_idle_core(struct rq *rq)
 	if (test_idle_cores(core, true))
 		goto unlock;
 
-	for_each_cpu(cpu, cpu_smt_mask(core)) {
+	for_each_cpu (cpu, cpu_smt_mask(core)) {
 		if (cpu == core)
 			continue;
 
@@ -6272,7 +6849,8 @@ void __update_idle_core(struct rq *rq)
  * there are no idle cores left in the system; tracked through
  * sd_llc->shared->has_idle_cores and enabled through update_idle_core() above.
  */
-static int select_idle_core(struct task_struct *p, int core, struct cpumask *cpus, int *idle_cpu)
+static int select_idle_core(struct task_struct *p, int core,
+			    struct cpumask *cpus, int *idle_cpu)
 {
 	bool idle = true;
 	int cpu;
@@ -6280,11 +6858,12 @@ static int select_idle_core(struct task_struct *p, int core, struct cpumask *cpu
 	if (!static_branch_likely(&sched_smt_present))
 		return __select_idle_cpu(core, p);
 
-	for_each_cpu(cpu, cpu_smt_mask(core)) {
+	for_each_cpu (cpu, cpu_smt_mask(core)) {
 		if (!available_idle_cpu(cpu)) {
 			idle = false;
 			if (*idle_cpu == -1) {
-				if (sched_idle_cpu(cpu) && cpumask_test_cpu(cpu, p->cpus_ptr)) {
+				if (sched_idle_cpu(cpu) &&
+				    cpumask_test_cpu(cpu, p->cpus_ptr)) {
 					*idle_cpu = cpu;
 					break;
 				}
@@ -6306,11 +6885,12 @@ static int select_idle_core(struct task_struct *p, int core, struct cpumask *cpu
 /*
  * Scan the local SMT mask for idle CPUs.
  */
-static int select_idle_smt(struct task_struct *p, struct sched_domain *sd, int target)
+static int select_idle_smt(struct task_struct *p, struct sched_domain *sd,
+			   int target)
 {
 	int cpu;
 
-	for_each_cpu(cpu, cpu_smt_mask(target)) {
+	for_each_cpu (cpu, cpu_smt_mask(target)) {
 		if (!cpumask_test_cpu(cpu, p->cpus_ptr) ||
 		    !cpumask_test_cpu(cpu, sched_domain_span(sd)))
 			continue;
@@ -6332,12 +6912,14 @@ static inline bool test_idle_cores(int cpu, bool def)
 	return def;
 }
 
-static inline int select_idle_core(struct task_struct *p, int core, struct cpumask *cpus, int *idle_cpu)
+static inline int select_idle_core(struct task_struct *p, int core,
+				   struct cpumask *cpus, int *idle_cpu)
 {
 	return __select_idle_cpu(core, p);
 }
 
-static inline int select_idle_smt(struct task_struct *p, struct sched_domain *sd, int target)
+static inline int select_idle_smt(struct task_struct *p,
+				  struct sched_domain *sd, int target)
 {
 	return -1;
 }
@@ -6349,7 +6931,8 @@ static inline int select_idle_smt(struct task_struct *p, struct sched_domain *sd
  * comparing the average scan cost (tracked in sd->avg_scan_cost) against the
  * average idle time for this rq (as found in rq->avg_idle).
  */
-static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, bool has_idle_core, int target)
+static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd,
+			   bool has_idle_core, int target)
 {
 	struct cpumask *cpus = this_cpu_cpumask_var_ptr(select_idle_mask);
 	int i, cpu, idle_cpu = -1, nr = INT_MAX;
@@ -6374,7 +6957,8 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, bool
 		 * predicted idle time.
 		 */
 		if (unlikely(this_rq->wake_stamp < now)) {
-			while (this_rq->wake_stamp < now && this_rq->wake_avg_idle) {
+			while (this_rq->wake_stamp < now &&
+			       this_rq->wake_avg_idle) {
 				this_rq->wake_stamp++;
 				this_rq->wake_avg_idle >>= 1;
 			}
@@ -6384,7 +6968,7 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, bool
 		avg_cost = this_sd->avg_scan_cost + 1;
 
 		span_avg = sd->span_weight * avg_idle;
-		if (span_avg > 4*avg_cost)
+		if (span_avg > 4 * avg_cost)
 			nr = div_u64(span_avg, avg_cost);
 		else
 			nr = 4;
@@ -6392,7 +6976,7 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, bool
 		time = cpu_clock(this);
 	}
 
-	for_each_cpu_wrap(cpu, cpus, target + 1) {
+	for_each_cpu_wrap (cpu, cpus, target + 1) {
 		if (has_idle_core) {
 			i = select_idle_core(p, cpu, cpus, &idle_cpu);
 			if ((unsigned int)i < nr_cpumask_bits)
@@ -6430,8 +7014,8 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, bool
  * the task fits. If no CPU is big enough, but there are idle ones, try to
  * maximize capacity.
  */
-static int
-select_idle_capacity(struct task_struct *p, struct sched_domain *sd, int target)
+static int select_idle_capacity(struct task_struct *p, struct sched_domain *sd,
+				int target)
 {
 	unsigned long task_util, best_cap = 0;
 	int cpu, best_cpu = -1;
@@ -6442,7 +7026,7 @@ select_idle_capacity(struct task_struct *p, struct sched_domain *sd, int target)
 
 	task_util = uclamp_task_util(p);
 
-	for_each_cpu_wrap(cpu, cpus, target) {
+	for_each_cpu_wrap (cpu, cpus, target) {
 		unsigned long cpu_cap = capacity_of(cpu);
 
 		if (!available_idle_cpu(cpu) && !sched_idle_cpu(cpu))
@@ -6511,10 +7095,8 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 	 * essentially a sync wakeup. An obvious example of this
 	 * pattern is IO completions.
 	 */
-	if (is_per_cpu_kthread(current) &&
-	    in_task() &&
-	    prev == smp_processor_id() &&
-	    this_rq()->nr_running <= 1 &&
+	if (is_per_cpu_kthread(current) && in_task() &&
+	    prev == smp_processor_id() && this_rq()->nr_running <= 1 &&
 	    asym_fits_capacity(task_util, prev)) {
 		return prev;
 	}
@@ -6522,10 +7104,10 @@ static int select_idle_sibling(struct task_struct *p, int prev, int target)
 	/* Check a recently used CPU as a potential idle candidate: */
 	recent_used_cpu = p->recent_used_cpu;
 	p->recent_used_cpu = prev;
-	if (recent_used_cpu != prev &&
-	    recent_used_cpu != target &&
+	if (recent_used_cpu != prev && recent_used_cpu != target &&
 	    cpus_share_cache(recent_used_cpu, target) &&
-	    (available_idle_cpu(recent_used_cpu) || sched_idle_cpu(recent_used_cpu)) &&
+	    (available_idle_cpu(recent_used_cpu) ||
+	     sched_idle_cpu(recent_used_cpu)) &&
 	    cpumask_test_cpu(p->recent_used_cpu, p->cpus_ptr) &&
 	    asym_fits_capacity(task_util, recent_used_cpu)) {
 		return recent_used_cpu;
@@ -6706,8 +7288,8 @@ static unsigned long cpu_util_next(int cpu, struct task_struct *p, int dst_cpu)
  * to compute what would be the energy if we decided to actually migrate that
  * task.
  */
-static long
-compute_energy(struct task_struct *p, int dst_cpu, struct perf_domain *pd)
+static long compute_energy(struct task_struct *p, int dst_cpu,
+			   struct perf_domain *pd)
 {
 	struct cpumask *pd_mask = perf_domain_span(pd);
 	unsigned long cpu_cap = arch_scale_cpu_capacity(cpumask_first(pd_mask));
@@ -6726,7 +7308,7 @@ compute_energy(struct task_struct *p, int dst_cpu, struct perf_domain *pd)
 	 * If an entire pd is outside of the current rd, it will not appear in
 	 * its pd list and will not be accounted by compute_energy().
 	 */
-	for_each_cpu_and(cpu, pd_mask, cpu_online_mask) {
+	for_each_cpu_and (cpu, pd_mask, cpu_online_mask) {
 		unsigned long util_freq = cpu_util_next(cpu, p, dst_cpu);
 		unsigned long cpu_util, util_running = util_freq;
 		struct task_struct *tsk = NULL;
@@ -6847,7 +7429,8 @@ static int find_energy_efficient_cpu(struct task_struct *p, int prev_cpu)
 		unsigned long base_energy_pd;
 		int max_spare_cap_cpu = -1;
 
-		for_each_cpu_and(cpu, perf_domain_span(pd), sched_domain_span(sd)) {
+		for_each_cpu_and (cpu, perf_domain_span(pd),
+				  sched_domain_span(sd)) {
 			if (!cpumask_test_cpu(cpu, p->cpus_ptr))
 				continue;
 
@@ -6936,8 +7519,8 @@ static int find_energy_efficient_cpu(struct task_struct *p, int prev_cpu)
  *
  * Returns the target CPU number.
  */
-static int
-select_task_rq_fair(struct task_struct *p, int prev_cpu, int wake_flags)
+static int select_task_rq_fair(struct task_struct *p, int prev_cpu,
+			       int wake_flags)
 {
 	int sync = (wake_flags & WF_SYNC) && !(current->flags & PF_EXITING);
 	struct sched_domain *tmp, *sd = NULL;
@@ -6961,11 +7544,13 @@ select_task_rq_fair(struct task_struct *p, int prev_cpu, int wake_flags)
 			new_cpu = prev_cpu;
 		}
 
-		want_affine = !wake_wide(p) && cpumask_test_cpu(cpu, p->cpus_ptr);
+		want_affine =
+			!wake_wide(p) && cpumask_test_cpu(cpu, p->cpus_ptr);
 	}
 
 	rcu_read_lock();
-	for_each_domain(cpu, tmp) {
+	for_each_domain(cpu, tmp)
+	{
 		/*
 		 * If both 'cpu' and 'prev_cpu' are part of this domain,
 		 * cpu is a valid SD_WAKE_AFFINE target.
@@ -6973,7 +7558,8 @@ select_task_rq_fair(struct task_struct *p, int prev_cpu, int wake_flags)
 		if (want_affine && (tmp->flags & SD_WAKE_AFFINE) &&
 		    cpumask_test_cpu(prev_cpu, sched_domain_span(tmp))) {
 			if (cpu != prev_cpu)
-				new_cpu = wake_affine(tmp, p, cpu, prev_cpu, sync);
+				new_cpu = wake_affine(tmp, p, cpu, prev_cpu,
+						      sync);
 
 			sd = NULL; /* Prefer wake_affine over balance flags */
 			break;
@@ -7071,8 +7657,8 @@ static void task_dead_fair(struct task_struct *p)
 	remove_entity_load_avg(&p->se);
 }
 
-static int
-balance_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
+static int balance_fair(struct rq *rq, struct task_struct *prev,
+			struct rq_flags *rf)
 {
 	if (rq->nr_running)
 		return 1;
@@ -7115,8 +7701,8 @@ static unsigned long wakeup_gran(struct sched_entity *se)
  *  w(c, s3) =  1
  *
  */
-static int
-wakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se)
+static int wakeup_preempt_entity(struct sched_entity *curr,
+				 struct sched_entity *se)
 {
 	s64 gran, vdiff = curr->vruntime - se->vruntime;
 
@@ -7132,7 +7718,8 @@ wakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se)
 
 static void set_last_buddy(struct sched_entity *se)
 {
-	for_each_sched_entity(se) {
+	for_each_sched_entity(se)
+	{
 		if (SCHED_WARN_ON(!se->on_rq))
 			return;
 		if (se_is_idle(se))
@@ -7143,7 +7730,8 @@ static void set_last_buddy(struct sched_entity *se)
 
 static void set_next_buddy(struct sched_entity *se)
 {
-	for_each_sched_entity(se) {
+	for_each_sched_entity(se)
+	{
 		if (SCHED_WARN_ON(!se->on_rq))
 			return;
 		if (se_is_idle(se))
@@ -7154,14 +7742,14 @@ static void set_next_buddy(struct sched_entity *se)
 
 static void set_skip_buddy(struct sched_entity *se)
 {
-	for_each_sched_entity(se)
-		cfs_rq_of(se)->skip = se;
+	for_each_sched_entity(se) cfs_rq_of(se)->skip = se;
 }
 
 /*
  * Preempt the current task with a newly woken task if needed:
  */
-static void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_flags)
+static void check_preempt_wakeup(struct rq *rq, struct task_struct *p,
+				 int wake_flags)
 {
 	struct task_struct *curr = rq->curr;
 	struct sched_entity *se = &curr->se, *pse = &p->se;
@@ -7209,7 +7797,8 @@ static void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_
 	 * Batch and idle tasks do not preempt non-idle tasks (their preemption
 	 * is driven by the tick):
 	 */
-	if (unlikely(p->policy != SCHED_NORMAL) || !sched_feat(WAKEUP_PREEMPTION))
+	if (unlikely(p->policy != SCHED_NORMAL) ||
+	    !sched_feat(WAKEUP_PREEMPTION))
 		return;
 
 	find_matching_se(&se, &pse);
@@ -7291,8 +7880,8 @@ static struct task_struct *pick_task_fair(struct rq *rq)
 }
 #endif
 
-struct task_struct *
-pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
+struct task_struct *pick_next_task_fair(struct rq *rq, struct task_struct *prev,
+					struct rq_flags *rf)
 {
 	struct cfs_rq *cfs_rq = &rq->cfs;
 	struct sched_entity *se;
@@ -7392,7 +7981,8 @@ pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf
 
 	p = task_of(se);
 
-done: __maybe_unused;
+done:
+	__maybe_unused;
 #ifdef CONFIG_SMP
 	/*
 	 * Move the next running task to the front of
@@ -7448,7 +8038,8 @@ static void put_prev_task_fair(struct rq *rq, struct task_struct *prev)
 	struct sched_entity *se = &prev->se;
 	struct cfs_rq *cfs_rq;
 
-	for_each_sched_entity(se) {
+	for_each_sched_entity(se)
+	{
 		cfs_rq = cfs_rq_of(se);
 		put_prev_entity(cfs_rq, se);
 	}
@@ -7625,7 +8216,7 @@ static bool yield_to_task_fair(struct rq *rq, struct task_struct *p)
  *      rewrite all of this once again.]
  */
 
-static unsigned long __read_mostly max_load_balance_interval = HZ/10;
+static unsigned long __read_mostly max_load_balance_interval = HZ / 10;
 
 enum fbq_type { regular, remote, all };
 
@@ -7674,37 +8265,37 @@ enum migration_type {
 	migrate_misfit
 };
 
-#define LBF_ALL_PINNED	0x01
-#define LBF_NEED_BREAK	0x02
-#define LBF_DST_PINNED  0x04
-#define LBF_SOME_PINNED	0x08
-#define LBF_ACTIVE_LB	0x10
+#define LBF_ALL_PINNED 0x01
+#define LBF_NEED_BREAK 0x02
+#define LBF_DST_PINNED 0x04
+#define LBF_SOME_PINNED 0x08
+#define LBF_ACTIVE_LB 0x10
 
 struct lb_env {
-	struct sched_domain	*sd;
+	struct sched_domain *sd;
 
-	struct rq		*src_rq;
-	int			src_cpu;
+	struct rq *src_rq;
+	int src_cpu;
 
-	int			dst_cpu;
-	struct rq		*dst_rq;
+	int dst_cpu;
+	struct rq *dst_rq;
 
-	struct cpumask		*dst_grpmask;
-	int			new_dst_cpu;
-	enum cpu_idle_type	idle;
-	long			imbalance;
+	struct cpumask *dst_grpmask;
+	int new_dst_cpu;
+	enum cpu_idle_type idle;
+	long imbalance;
 	/* The set of CPUs under consideration for load-balancing */
-	struct cpumask		*cpus;
+	struct cpumask *cpus;
 
-	unsigned int		flags;
+	unsigned int flags;
 
-	unsigned int		loop;
-	unsigned int		loop_break;
-	unsigned int		loop_max;
+	unsigned int loop;
+	unsigned int loop_break;
+	unsigned int loop_max;
 
-	enum fbq_type		fbq_type;
-	enum migration_type	migration_type;
-	struct list_head	tasks;
+	enum fbq_type fbq_type;
+	enum migration_type migration_type;
+	struct list_head tasks;
 };
 
 /*
@@ -7730,8 +8321,8 @@ static int task_hot(struct task_struct *p, struct lb_env *env)
 	 * Buddy candidates are cache hot:
 	 */
 	if (sched_feat(CACHE_HOT_BUDDY) && env->dst_rq->nr_running &&
-			(&p->se == cfs_rq_of(&p->se)->next ||
-			 &p->se == cfs_rq_of(&p->se)->last))
+	    (&p->se == cfs_rq_of(&p->se)->next ||
+	     &p->se == cfs_rq_of(&p->se)->last))
 		return 1;
 
 	if (sysctl_sched_migration_cost == -1)
@@ -7806,7 +8397,7 @@ static int migrate_degrades_locality(struct task_struct *p, struct lb_env *env)
 
 #else
 static inline int migrate_degrades_locality(struct task_struct *p,
-					     struct lb_env *env)
+					    struct lb_env *env)
 {
 	return -1;
 }
@@ -7815,8 +8406,7 @@ static inline int migrate_degrades_locality(struct task_struct *p,
 /*
  * can_migrate_task - may task p from runqueue rq be migrated to this_cpu?
  */
-static
-int can_migrate_task(struct task_struct *p, struct lb_env *env)
+static int can_migrate_task(struct task_struct *p, struct lb_env *env)
 {
 	int tsk_cache_hot;
 
@@ -7858,7 +8448,7 @@ int can_migrate_task(struct task_struct *p, struct lb_env *env)
 			return 0;
 
 		/* Prevent to re-select dst_cpu via env's CPUs: */
-		for_each_cpu_and(cpu, env->dst_grpmask, env->cpus) {
+		for_each_cpu_and (cpu, env->dst_grpmask, env->cpus) {
 			if (cpumask_test_cpu(cpu, p->cpus_ptr)) {
 				env->flags |= LBF_DST_PINNED;
 				env->new_dst_cpu = cpu;
@@ -7927,8 +8517,8 @@ static struct task_struct *detach_one_task(struct lb_env *env)
 
 	lockdep_assert_rq_held(env->src_rq);
 
-	list_for_each_entry_reverse(p,
-			&env->src_rq->cfs_tasks, se.group_node) {
+	list_for_each_entry_reverse (p, &env->src_rq->cfs_tasks,
+				     se.group_node) {
 		if (!can_migrate_task(p, env))
 			continue;
 
@@ -8011,8 +8601,8 @@ static int detach_tasks(struct lb_env *env)
 			 */
 			load = max_t(unsigned long, task_h_load(p), 1);
 
-			if (sched_feat(LB_MIN) &&
-			    load < 16 && !env->sd->nr_balance_failed)
+			if (sched_feat(LB_MIN) && load < 16 &&
+			    !env->sd->nr_balance_failed)
 				goto next;
 
 			/*
@@ -8021,7 +8611,8 @@ static int detach_tasks(struct lb_env *env)
 			 * scheduler fails to find a good waiting task to
 			 * migrate.
 			 */
-			if (shr_bound(load, env->sd->nr_balance_failed) > env->imbalance)
+			if (shr_bound(load, env->sd->nr_balance_failed) >
+			    env->imbalance)
 				goto next;
 
 			env->imbalance -= load;
@@ -8072,7 +8663,7 @@ static int detach_tasks(struct lb_env *env)
 			break;
 
 		continue;
-next:
+	next:
 		list_move(&p->se.group_node, tasks);
 	}
 
@@ -8177,10 +8768,20 @@ static inline void update_blocked_load_status(struct rq *rq, bool has_blocked)
 		rq->has_blocked_load = 0;
 }
 #else
-static inline bool cfs_rq_has_blocked(struct cfs_rq *cfs_rq) { return false; }
-static inline bool others_have_blocked(struct rq *rq) { return false; }
-static inline void update_blocked_load_tick(struct rq *rq) {}
-static inline void update_blocked_load_status(struct rq *rq, bool has_blocked) {}
+static inline bool cfs_rq_has_blocked(struct cfs_rq *cfs_rq)
+{
+	return false;
+}
+static inline bool others_have_blocked(struct rq *rq)
+{
+	return false;
+}
+static inline void update_blocked_load_tick(struct rq *rq)
+{
+}
+static inline void update_blocked_load_status(struct rq *rq, bool has_blocked)
+{
+}
 #endif
 
 static bool __update_blocked_others(struct rq *rq, bool *done)
@@ -8198,10 +8799,12 @@ static bool __update_blocked_others(struct rq *rq, bool *done)
 
 	thermal_pressure = arch_scale_thermal_pressure(cpu_of(rq));
 
-	decayed = update_rt_rq_load_avg(now, rq, curr_class == &rt_sched_class) |
-		  update_dl_rq_load_avg(now, rq, curr_class == &dl_sched_class) |
-		  update_thermal_load_avg(rq_clock_thermal(rq), rq, thermal_pressure) |
-		  update_irq_load_avg(rq, 0);
+	decayed =
+		update_rt_rq_load_avg(now, rq, curr_class == &rt_sched_class) |
+		update_dl_rq_load_avg(now, rq, curr_class == &dl_sched_class) |
+		update_thermal_load_avg(rq_clock_thermal(rq), rq,
+					thermal_pressure) |
+		update_irq_load_avg(rq, 0);
 
 	if (others_have_blocked(rq))
 		*done = false;
@@ -8221,7 +8824,8 @@ static bool __update_blocked_fair(struct rq *rq, bool *done)
 	 * Iterates the task_group tree in a bottom up fashion, see
 	 * list_add_leaf_cfs_rq() for details.
 	 */
-	for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos) {
+	for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos)
+	{
 		struct sched_entity *se;
 
 		if (update_cfs_rq_load_avg(cfs_rq_clock_pelt(cfs_rq), cfs_rq)) {
@@ -8267,7 +8871,8 @@ static void update_cfs_rq_h_load(struct cfs_rq *cfs_rq)
 		return;
 
 	WRITE_ONCE(cfs_rq->h_load_next, NULL);
-	for_each_sched_entity(se) {
+	for_each_sched_entity(se)
+	{
 		cfs_rq = cfs_rq_of(se);
 		WRITE_ONCE(cfs_rq->h_load_next, se);
 		if (cfs_rq->last_h_load_update == now)
@@ -8282,7 +8887,7 @@ static void update_cfs_rq_h_load(struct cfs_rq *cfs_rq)
 	while ((se = READ_ONCE(cfs_rq->h_load_next)) != NULL) {
 		load = cfs_rq->h_load;
 		load = div64_ul(load * se->avg.load_avg,
-			cfs_rq_load_avg(cfs_rq) + 1);
+				cfs_rq_load_avg(cfs_rq) + 1);
 		cfs_rq = group_cfs_rq(se);
 		cfs_rq->h_load = load;
 		cfs_rq->last_h_load_update = now;
@@ -8344,15 +8949,19 @@ struct sg_lb_stats {
 	unsigned long avg_load; /*Avg load across the CPUs of the group */
 	unsigned long group_load; /* Total load over the CPUs of the group */
 	unsigned long group_capacity;
-	unsigned long group_util; /* Total utilization over the CPUs of the group */
-	unsigned long group_runnable; /* Total runnable time over the CPUs of the group */
+	unsigned long
+		group_util; /* Total utilization over the CPUs of the group */
+	unsigned long
+		group_runnable; /* Total runnable time over the CPUs of the group */
 	unsigned int sum_nr_running; /* Nr of tasks running in the group */
 	unsigned int sum_h_nr_running; /* Nr of CFS tasks running in the group */
 	unsigned int idle_cpus;
 	unsigned int group_weight;
 	enum group_type group_type;
-	unsigned int group_asym_packing; /* Tasks should be moved to preferred CPU */
-	unsigned long group_misfit_task_load; /* A CPU has a task too big for its capacity */
+	unsigned int
+		group_asym_packing; /* Tasks should be moved to preferred CPU */
+	unsigned long
+		group_misfit_task_load; /* A CPU has a task too big for its capacity */
 #ifdef CONFIG_NUMA_BALANCING
 	unsigned int nr_numa_running;
 	unsigned int nr_preferred_running;
@@ -8364,15 +8973,15 @@ struct sg_lb_stats {
  *		 during load balancing.
  */
 struct sd_lb_stats {
-	struct sched_group *busiest;	/* Busiest group in this sd */
-	struct sched_group *local;	/* Local group in this sd */
-	unsigned long total_load;	/* Total load of all groups in sd */
-	unsigned long total_capacity;	/* Total capacity of all groups in sd */
-	unsigned long avg_load;	/* Average load across all groups in sd */
+	struct sched_group *busiest; /* Busiest group in this sd */
+	struct sched_group *local; /* Local group in this sd */
+	unsigned long total_load; /* Total load of all groups in sd */
+	unsigned long total_capacity; /* Total capacity of all groups in sd */
+	unsigned long avg_load; /* Average load across all groups in sd */
 	unsigned int prefer_sibling; /* tasks should go to sibling first */
 
-	struct sg_lb_stats busiest_stat;/* Statistics of the busiest group */
-	struct sg_lb_stats local_stat;	/* Statistics of the local group */
+	struct sg_lb_stats busiest_stat; /* Statistics of the busiest group */
+	struct sg_lb_stats local_stat; /* Statistics of the local group */
 };
 
 static inline void init_sd_lb_stats(struct sd_lb_stats *sds)
@@ -8470,14 +9079,14 @@ void update_group_capacity(struct sched_domain *sd, int cpu)
 		 * span the current group.
 		 */
 
-		for_each_cpu(cpu, sched_group_span(sdg)) {
+		for_each_cpu (cpu, sched_group_span(sdg)) {
 			unsigned long cpu_cap = capacity_of(cpu);
 
 			capacity += cpu_cap;
 			min_capacity = min(cpu_cap, min_capacity);
 			max_capacity = max(cpu_cap, max_capacity);
 		}
-	} else  {
+	} else {
 		/*
 		 * !SD_OVERLAP domains can assume that child groups
 		 * span the current group.
@@ -8504,11 +9113,10 @@ void update_group_capacity(struct sched_domain *sd, int cpu)
  * activity. The imbalance_pct is used for the threshold.
  * Return true is the capacity is reduced
  */
-static inline int
-check_cpu_capacity(struct rq *rq, struct sched_domain *sd)
+static inline int check_cpu_capacity(struct rq *rq, struct sched_domain *sd)
 {
 	return ((rq->cpu_capacity * sd->imbalance_pct) <
-				(rq->cpu_capacity_orig * 100));
+		(rq->cpu_capacity_orig * 100));
 }
 
 /*
@@ -8519,8 +9127,8 @@ check_cpu_capacity(struct rq *rq, struct sched_domain *sd)
 static inline int check_misfit_status(struct rq *rq, struct sched_domain *sd)
 {
 	return rq->misfit_task_load &&
-		(rq->cpu_capacity_orig < rq->rd->max_cpu_capacity ||
-		 check_cpu_capacity(rq, sd));
+	       (rq->cpu_capacity_orig < rq->rd->max_cpu_capacity ||
+		check_cpu_capacity(rq, sd));
 }
 
 /*
@@ -8569,18 +9177,16 @@ static inline int sg_imbalanced(struct sched_group *group)
  * As an example, an available capacity of 1% can appear but it doesn't make
  * any benefit for the load balance.
  */
-static inline bool
-group_has_capacity(unsigned int imbalance_pct, struct sg_lb_stats *sgs)
+static inline bool group_has_capacity(unsigned int imbalance_pct,
+				      struct sg_lb_stats *sgs)
 {
 	if (sgs->sum_nr_running < sgs->group_weight)
 		return true;
 
-	if ((sgs->group_capacity * imbalance_pct) <
-			(sgs->group_runnable * 100))
+	if ((sgs->group_capacity * imbalance_pct) < (sgs->group_runnable * 100))
 		return false;
 
-	if ((sgs->group_capacity * 100) >
-			(sgs->group_util * imbalance_pct))
+	if ((sgs->group_capacity * 100) > (sgs->group_util * imbalance_pct))
 		return true;
 
 	return false;
@@ -8594,27 +9200,24 @@ group_has_capacity(unsigned int imbalance_pct, struct sg_lb_stats *sgs)
  *  overloaded so both group_has_capacity and group_is_overloaded return
  *  false.
  */
-static inline bool
-group_is_overloaded(unsigned int imbalance_pct, struct sg_lb_stats *sgs)
+static inline bool group_is_overloaded(unsigned int imbalance_pct,
+				       struct sg_lb_stats *sgs)
 {
 	if (sgs->sum_nr_running <= sgs->group_weight)
 		return false;
 
-	if ((sgs->group_capacity * 100) <
-			(sgs->group_util * imbalance_pct))
+	if ((sgs->group_capacity * 100) < (sgs->group_util * imbalance_pct))
 		return true;
 
-	if ((sgs->group_capacity * imbalance_pct) <
-			(sgs->group_runnable * 100))
+	if ((sgs->group_capacity * imbalance_pct) < (sgs->group_runnable * 100))
 		return true;
 
 	return false;
 }
 
-static inline enum
-group_type group_classify(unsigned int imbalance_pct,
-			  struct sched_group *group,
-			  struct sg_lb_stats *sgs)
+static inline enum group_type group_classify(unsigned int imbalance_pct,
+					     struct sched_group *group,
+					     struct sg_lb_stats *sgs)
 {
 	if (group_is_overloaded(imbalance_pct, sgs))
 		return group_overloaded;
@@ -8692,8 +9295,8 @@ static bool asym_smt_can_pull_tasks(int dst_cpu, struct sd_lb_stats *sds,
 	/* @dst_cpu has SMT siblings. */
 
 	if (sg_is_smt) {
-		int local_busy_cpus = sds->local->group_weight -
-				      sds->local_stat.idle_cpus;
+		int local_busy_cpus =
+			sds->local->group_weight - sds->local_stat.idle_cpus;
 		int busy_cpus_delta = sg_busy_cpus - local_busy_cpus;
 
 		if (busy_cpus_delta == 1)
@@ -8717,9 +9320,9 @@ static bool asym_smt_can_pull_tasks(int dst_cpu, struct sd_lb_stats *sds,
 #endif
 }
 
-static inline bool
-sched_asym(struct lb_env *env, struct sd_lb_stats *sds,  struct sg_lb_stats *sgs,
-	   struct sched_group *group)
+static inline bool sched_asym(struct lb_env *env, struct sd_lb_stats *sds,
+			      struct sg_lb_stats *sgs,
+			      struct sched_group *group)
 {
 	/* Only do SMT checks if either local or candidate have SMT siblings */
 	if ((sds->local->flags & SD_SHARE_CPUCAPACITY) ||
@@ -8740,8 +9343,7 @@ sched_asym(struct lb_env *env, struct sd_lb_stats *sds,  struct sg_lb_stats *sgs
 static inline void update_sg_lb_stats(struct lb_env *env,
 				      struct sd_lb_stats *sds,
 				      struct sched_group *group,
-				      struct sg_lb_stats *sgs,
-				      int *sg_status)
+				      struct sg_lb_stats *sgs, int *sg_status)
 {
 	int i, nr_running, local_group;
 
@@ -8749,7 +9351,7 @@ static inline void update_sg_lb_stats(struct lb_env *env,
 
 	local_group = group == sds->local;
 
-	for_each_cpu_and(i, sched_group_span(group), env->cpus) {
+	for_each_cpu_and (i, sched_group_span(group), env->cpus) {
 		struct rq *rq = cpu_rq(i);
 
 		sgs->group_load += cpu_load(rq);
@@ -8822,8 +9424,7 @@ static inline void update_sg_lb_stats(struct lb_env *env,
  * Return: %true if @sg is a busier group than the previously selected
  * busiest group. %false otherwise.
  */
-static bool update_sd_pick_busiest(struct lb_env *env,
-				   struct sd_lb_stats *sds,
+static bool update_sd_pick_busiest(struct lb_env *env, struct sd_lb_stats *sds,
 				   struct sched_group *sg,
 				   struct sg_lb_stats *sgs)
 {
@@ -8840,7 +9441,8 @@ static bool update_sd_pick_busiest(struct lb_env *env,
 	 * internally or be covered by avg_load imbalance (eventually).
 	 */
 	if (sgs->group_type == group_misfit_task &&
-	    (!capacity_greater(capacity_of(env->dst_cpu), sg->sgc->max_capacity) ||
+	    (!capacity_greater(capacity_of(env->dst_cpu),
+			       sg->sgc->max_capacity) ||
 	     sds->local_stat.group_type != group_has_spare))
 		return false;
 
@@ -8871,7 +9473,8 @@ static bool update_sd_pick_busiest(struct lb_env *env,
 
 	case group_asym_packing:
 		/* Prefer to move from lowest priority CPU's work */
-		if (sched_asym_prefer(sg->asym_prefer_cpu, sds->busiest->asym_prefer_cpu))
+		if (sched_asym_prefer(sg->asym_prefer_cpu,
+				      sds->busiest->asym_prefer_cpu))
 			return false;
 		break;
 
@@ -8880,7 +9483,8 @@ static bool update_sd_pick_busiest(struct lb_env *env,
 		 * If we have more than one misfit sg go with the biggest
 		 * misfit.
 		 */
-		if (sgs->group_misfit_task_load < busiest->group_misfit_task_load)
+		if (sgs->group_misfit_task_load <
+		    busiest->group_misfit_task_load)
 			return false;
 		break;
 
@@ -8924,7 +9528,8 @@ static bool update_sd_pick_busiest(struct lb_env *env,
 	 */
 	if ((env->sd->flags & SD_ASYM_CPUCAPACITY) &&
 	    (sgs->group_type <= group_fully_busy) &&
-	    (capacity_greater(sg->sgc->min_capacity, capacity_of(env->dst_cpu))))
+	    (capacity_greater(sg->sgc->min_capacity,
+			      capacity_of(env->dst_cpu))))
 		return false;
 
 	return true;
@@ -8960,7 +9565,6 @@ static inline enum fbq_type fbq_classify_rq(struct rq *rq)
 }
 #endif /* CONFIG_NUMA_BALANCING */
 
-
 struct sg_lb_stats;
 
 /*
@@ -8993,7 +9597,7 @@ static int idle_cpu_without(int cpu, struct task_struct *p)
 	if (rq->curr != rq->idle && rq->curr != p)
 		return 0;
 
-	/*
+		/*
 	 * rq->nr_running can't be used but an updated version without the
 	 * impact of p on cpu must be used instead. The updated nr_running
 	 * be computed and tested before calling idle_cpu_without().
@@ -9023,7 +9627,7 @@ static inline void update_sg_wakeup_stats(struct sched_domain *sd,
 
 	memset(sgs, 0, sizeof(*sgs));
 
-	for_each_cpu(i, sched_group_span(group)) {
+	for_each_cpu (i, sched_group_span(group)) {
 		struct rq *rq = cpu_rq(i);
 		unsigned int local;
 
@@ -9041,7 +9645,6 @@ static inline void update_sg_wakeup_stats(struct sched_domain *sd,
 		 */
 		if (!nr_running && idle_cpu_without(i, p))
 			sgs->idle_cpus++;
-
 	}
 
 	/* Check if task fits in the group */
@@ -9061,7 +9664,7 @@ static inline void update_sg_wakeup_stats(struct sched_domain *sd,
 	 * overloaded
 	 */
 	if (sgs->group_type == group_fully_busy ||
-		sgs->group_type == group_overloaded)
+	    sgs->group_type == group_overloaded)
 		sgs->avg_load = (sgs->group_load * SCHED_CAPACITY_SCALE) /
 				sgs->group_capacity;
 }
@@ -9108,7 +9711,7 @@ static bool update_pick_idlest(struct sched_group *idlest,
 
 		/* Select group with lowest group_util */
 		if (idlest_sgs->idle_cpus == sgs->idle_cpus &&
-			idlest_sgs->group_util <= sgs->group_util)
+		    idlest_sgs->group_util <= sgs->group_util)
 			return false;
 
 		break;
@@ -9141,24 +9744,23 @@ find_idlest_group(struct sched_domain *sd, struct task_struct *p, int this_cpu)
 	struct sg_lb_stats *sgs;
 	unsigned long imbalance;
 	struct sg_lb_stats idlest_sgs = {
-			.avg_load = UINT_MAX,
-			.group_type = group_overloaded,
+		.avg_load = UINT_MAX,
+		.group_type = group_overloaded,
 	};
 
 	do {
 		int local_group;
 
 		/* Skip over this group if it has no CPUs allowed */
-		if (!cpumask_intersects(sched_group_span(group),
-					p->cpus_ptr))
+		if (!cpumask_intersects(sched_group_span(group), p->cpus_ptr))
 			continue;
 
 		/* Skip over this group if no cookie matched */
 		if (!sched_group_cookie_match(cpu_rq(this_cpu), p, group))
 			continue;
 
-		local_group = cpumask_test_cpu(this_cpu,
-					       sched_group_span(group));
+		local_group =
+			cpumask_test_cpu(this_cpu, sched_group_span(group));
 
 		if (local_group) {
 			sgs = &local_sgs;
@@ -9169,14 +9771,14 @@ find_idlest_group(struct sched_domain *sd, struct task_struct *p, int this_cpu)
 
 		update_sg_wakeup_stats(sd, group, sgs, p);
 
-		if (!local_group && update_pick_idlest(idlest, &idlest_sgs, group, sgs)) {
+		if (!local_group &&
+		    update_pick_idlest(idlest, &idlest_sgs, group, sgs)) {
 			idlest = group;
 			idlest_sgs = *sgs;
 		}
 
 	} while (group = group->next, group != sd->groups);
 
-
 	/* There is no idlest group to push tasks to */
 	if (!idlest)
 		return NULL;
@@ -9205,7 +9807,7 @@ find_idlest_group(struct sched_domain *sd, struct task_struct *p, int this_cpu)
 
 		/* Calculate allowed imbalance based on load */
 		imbalance = scale_load_down(NICE_0_LOAD) *
-				(sd->imbalance_pct-100) / 100;
+			    (sd->imbalance_pct - 100) / 100;
 
 		/*
 		 * When comparing groups across NUMA domains, it's possible for
@@ -9227,7 +9829,8 @@ find_idlest_group(struct sched_domain *sd, struct task_struct *p, int this_cpu)
 		if (idlest_sgs.avg_load >= (local_sgs.avg_load + imbalance))
 			return NULL;
 
-		if (100 * local_sgs.avg_load <= sd->imbalance_pct * idlest_sgs.avg_load)
+		if (100 * local_sgs.avg_load <=
+		    sd->imbalance_pct * idlest_sgs.avg_load)
 			return NULL;
 		break;
 
@@ -9264,7 +9867,8 @@ find_idlest_group(struct sched_domain *sd, struct task_struct *p, int this_cpu)
 			 * allowed. If there is a real need of migration,
 			 * periodic load balance will take care of it.
 			 */
-			if (allow_numa_imbalance(local_sgs.sum_nr_running + 1, sd->imb_numa_nr))
+			if (allow_numa_imbalance(local_sgs.sum_nr_running + 1,
+						 sd->imb_numa_nr))
 				return NULL;
 		}
 
@@ -9288,7 +9892,8 @@ find_idlest_group(struct sched_domain *sd, struct task_struct *p, int this_cpu)
  * @sds: variable to hold the statistics for this sched_domain.
  */
 
-static inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sds)
+static inline void update_sd_lb_stats(struct lb_env *env,
+				      struct sd_lb_stats *sds)
 {
 	struct sched_domain *child = env->sd->child;
 	struct sched_group *sg = env->sd->groups;
@@ -9300,7 +9905,8 @@ static inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sd
 		struct sg_lb_stats *sgs = &tmp_sgs;
 		int local_group;
 
-		local_group = cpumask_test_cpu(env->dst_cpu, sched_group_span(sg));
+		local_group =
+			cpumask_test_cpu(env->dst_cpu, sched_group_span(sg));
 		if (local_group) {
 			sds->local = sg;
 			sgs = local;
@@ -9315,13 +9921,12 @@ static inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sd
 		if (local_group)
 			goto next_group;
 
-
 		if (update_sd_pick_busiest(env, sds, sg, sgs)) {
 			sds->busiest = sg;
 			sds->busiest_stat = *sgs;
 		}
 
-next_group:
+	next_group:
 		/* Now, start updating sd_lb_stats */
 		sds->total_load += sgs->group_load;
 		sds->total_capacity += sgs->group_capacity;
@@ -9332,7 +9937,6 @@ static inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sd
 	/* Tag domain that child domain prefers tasks go to siblings first */
 	sds->prefer_sibling = child && child->flags & SD_PREFER_SIBLING;
 
-
 	if (env->sd->flags & SD_NUMA)
 		env->fbq_type = fbq_classify_group(&sds->busiest_stat);
 
@@ -9355,8 +9959,8 @@ static inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sd
 
 #define NUMA_IMBALANCE_MIN 2
 
-static inline long adjust_numa_imbalance(int imbalance,
-				int dst_running, int imb_numa_nr)
+static inline long adjust_numa_imbalance(int imbalance, int dst_running,
+					 int imb_numa_nr)
 {
 	if (!allow_numa_imbalance(dst_running, imb_numa_nr))
 		return imbalance;
@@ -9377,7 +9981,8 @@ static inline long adjust_numa_imbalance(int imbalance,
  * @env: load balance environment
  * @sds: statistics of the sched_domain whose imbalance is to be calculated.
  */
-static inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *sds)
+static inline void calculate_imbalance(struct lb_env *env,
+				       struct sd_lb_stats *sds)
 {
 	struct sg_lb_stats *local, *busiest;
 
@@ -9429,8 +10034,9 @@ static inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *s
 			 * system.
 			 */
 			env->migration_type = migrate_util;
-			env->imbalance = max(local->group_capacity, local->group_util) -
-					 local->group_util;
+			env->imbalance =
+				max(local->group_capacity, local->group_util) -
+				local->group_util;
 
 			/*
 			 * In some cases, the group's utilization is max or even
@@ -9457,20 +10063,22 @@ static inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *s
 			lsub_positive(&nr_diff, local->sum_nr_running);
 			env->imbalance = nr_diff >> 1;
 		} else {
-
 			/*
 			 * If there is no overload, we just want to even the number of
 			 * idle cpus.
 			 */
 			env->migration_type = migrate_task;
-			env->imbalance = max_t(long, 0, (local->idle_cpus -
-						 busiest->idle_cpus) >> 1);
+			env->imbalance = max_t(
+				long, 0,
+				(local->idle_cpus - busiest->idle_cpus) >> 1);
 		}
 
 		/* Consider allowing a small imbalance between NUMA groups */
 		if (env->sd->flags & SD_NUMA) {
-			env->imbalance = adjust_numa_imbalance(env->imbalance,
-				local->sum_nr_running + 1, env->sd->imb_numa_nr);
+			env->imbalance =
+				adjust_numa_imbalance(env->imbalance,
+						      local->sum_nr_running + 1,
+						      env->sd->imb_numa_nr);
 		}
 
 		return;
@@ -9510,10 +10118,11 @@ static inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *s
 	 * the minimum possible imbalance.
 	 */
 	env->migration_type = migrate_load;
-	env->imbalance = min(
-		(busiest->avg_load - sds->avg_load) * busiest->group_capacity,
-		(sds->avg_load - local->avg_load) * local->group_capacity
-	) / SCHED_CAPACITY_SCALE;
+	env->imbalance =
+		min((busiest->avg_load - sds->avg_load) *
+			    busiest->group_capacity,
+		    (sds->avg_load - local->avg_load) * local->group_capacity) /
+		SCHED_CAPACITY_SCALE;
 }
 
 /******* find_busiest_group() helpers end here *********************/
@@ -9612,7 +10221,7 @@ static struct sched_group *find_busiest_group(struct lb_env *env)
 
 		/* XXX broken for overlapping NUMA groups */
 		sds.avg_load = (sds.total_load * SCHED_CAPACITY_SCALE) /
-				sds.total_capacity;
+			       sds.total_capacity;
 
 		/*
 		 * Don't pull any tasks if this group is already above the
@@ -9626,7 +10235,7 @@ static struct sched_group *find_busiest_group(struct lb_env *env)
 		 * conservative.
 		 */
 		if (100 * busiest->avg_load <=
-				env->sd->imbalance_pct * local->avg_load)
+		    env->sd->imbalance_pct * local->avg_load)
 			goto out_balanced;
 	}
 
@@ -9685,7 +10294,7 @@ static struct rq *find_busiest_queue(struct lb_env *env,
 	unsigned int busiest_nr = 0;
 	int i;
 
-	for_each_cpu_and(i, sched_group_span(group), env->cpus) {
+	for_each_cpu_and (i, sched_group_span(group), env->cpus) {
 		unsigned long capacity, load, util;
 		unsigned int nr_running;
 		enum fbq_type rt;
@@ -9734,8 +10343,7 @@ static struct rq *find_busiest_queue(struct lb_env *env,
 
 		/* Make sure we only pull tasks from a CPU of lower priority */
 		if ((env->sd->flags & SD_ASYM_PACKING) &&
-		    sched_asym_prefer(i, env->dst_cpu) &&
-		    nr_running == 1)
+		    sched_asym_prefer(i, env->dst_cpu) && nr_running == 1)
 			continue;
 
 		switch (env->migration_type) {
@@ -9805,7 +10413,6 @@ static struct rq *find_busiest_queue(struct lb_env *env,
 			}
 
 			break;
-
 		}
 	}
 
@@ -9816,22 +10423,21 @@ static struct rq *find_busiest_queue(struct lb_env *env,
  * Max backoff if we encounter pinned tasks. Pretty arbitrary value, but
  * so long as it is large enough.
  */
-#define MAX_PINNED_INTERVAL	512
+#define MAX_PINNED_INTERVAL 512
 
-static inline bool
-asym_active_balance(struct lb_env *env)
+static inline bool asym_active_balance(struct lb_env *env)
 {
 	/*
 	 * ASYM_PACKING needs to force migrate tasks from busy but
 	 * lower priority CPUs in order to pack all tasks in the
 	 * highest priority CPUs.
 	 */
-	return env->idle != CPU_NOT_IDLE && (env->sd->flags & SD_ASYM_PACKING) &&
+	return env->idle != CPU_NOT_IDLE &&
+	       (env->sd->flags & SD_ASYM_PACKING) &&
 	       sched_asym_prefer(env->dst_cpu, env->src_cpu);
 }
 
-static inline bool
-imbalanced_active_balance(struct lb_env *env)
+static inline bool imbalanced_active_balance(struct lb_env *env)
 {
 	struct sched_domain *sd = env->sd;
 
@@ -9841,7 +10447,7 @@ imbalanced_active_balance(struct lb_env *env)
 	 * threads on a system with spare capacity
 	 */
 	if ((env->migration_type == migrate_task) &&
-	    (sd->nr_balance_failed > sd->cache_nice_tries+2))
+	    (sd->nr_balance_failed > sd->cache_nice_tries + 2))
 		return 1;
 
 	return 0;
@@ -9866,7 +10472,8 @@ static int need_active_balance(struct lb_env *env)
 	if ((env->idle != CPU_NOT_IDLE) &&
 	    (env->src_rq->cfs.h_nr_running == 1)) {
 		if ((check_cpu_capacity(env->src_rq, sd)) &&
-		    (capacity_of(env->src_cpu)*sd->imbalance_pct < capacity_of(env->dst_cpu)*100))
+		    (capacity_of(env->src_cpu) * sd->imbalance_pct <
+		     capacity_of(env->dst_cpu) * 100))
 			return 1;
 	}
 
@@ -9898,7 +10505,7 @@ static int should_we_balance(struct lb_env *env)
 		return 1;
 
 	/* Try to find first idle CPU */
-	for_each_cpu_and(cpu, group_balance_mask(sg), env->cpus) {
+	for_each_cpu_and (cpu, group_balance_mask(sg), env->cpus) {
 		if (!idle_cpu(cpu))
 			continue;
 
@@ -9926,15 +10533,15 @@ static int load_balance(int this_cpu, struct rq *this_rq,
 	struct cpumask *cpus = this_cpu_cpumask_var_ptr(load_balance_mask);
 
 	struct lb_env env = {
-		.sd		= sd,
-		.dst_cpu	= this_cpu,
-		.dst_rq		= this_rq,
-		.dst_grpmask    = sched_group_span(sd->groups),
-		.idle		= idle,
-		.loop_break	= sched_nr_migrate_break,
-		.cpus		= cpus,
-		.fbq_type	= all,
-		.tasks		= LIST_HEAD_INIT(env.tasks),
+		.sd = sd,
+		.dst_cpu = this_cpu,
+		.dst_rq = this_rq,
+		.dst_grpmask = sched_group_span(sd->groups),
+		.idle = idle,
+		.loop_break = sched_nr_migrate_break,
+		.cpus = cpus,
+		.fbq_type = all,
+		.tasks = LIST_HEAD_INIT(env.tasks),
 	};
 
 	cpumask_and(cpus, sched_domain_span(sd), cpu_active_mask);
@@ -9976,9 +10583,10 @@ static int load_balance(int this_cpu, struct rq *this_rq,
 		 * still unbalanced. ld_moved simply stays zero, so it is
 		 * correctly treated as an imbalance.
 		 */
-		env.loop_max  = min(sysctl_sched_nr_migrate, busiest->nr_running);
+		env.loop_max =
+			min(sysctl_sched_nr_migrate, busiest->nr_running);
 
-more_balance:
+	more_balance:
 		rq_lock_irqsave(busiest, &rf);
 		update_rq_clock(busiest);
 
@@ -10030,15 +10638,14 @@ static int load_balance(int this_cpu, struct rq *this_rq,
 		 * excess load moved.
 		 */
 		if ((env.flags & LBF_DST_PINNED) && env.imbalance > 0) {
-
 			/* Prevent to re-select dst_cpu via env's CPUs */
 			__cpumask_clear_cpu(env.dst_cpu, env.cpus);
 
-			env.dst_rq	 = cpu_rq(env.new_dst_cpu);
-			env.dst_cpu	 = env.new_dst_cpu;
-			env.flags	&= ~LBF_DST_PINNED;
-			env.loop	 = 0;
-			env.loop_break	 = sched_nr_migrate_break;
+			env.dst_rq = cpu_rq(env.new_dst_cpu);
+			env.dst_cpu = env.new_dst_cpu;
+			env.flags &= ~LBF_DST_PINNED;
+			env.loop = 0;
+			env.loop_break = sched_nr_migrate_break;
 
 			/*
 			 * Go back to "more_balance" rather than "redo" since we
@@ -10051,7 +10658,8 @@ static int load_balance(int this_cpu, struct rq *this_rq,
 		 * We failed to reach balance because of affinity.
 		 */
 		if (sd_parent) {
-			int *group_imbalance = &sd_parent->groups->sgc->imbalance;
+			int *group_imbalance =
+				&sd_parent->groups->sgc->imbalance;
 
 			if ((env.flags & LBF_SOME_PINNED) && env.imbalance > 0)
 				*group_imbalance = 1;
@@ -10098,7 +10706,8 @@ static int load_balance(int this_cpu, struct rq *this_rq,
 			 * if the curr task on busiest CPU can't be
 			 * moved to this_cpu:
 			 */
-			if (!cpumask_test_cpu(this_cpu, busiest->curr->cpus_ptr)) {
+			if (!cpumask_test_cpu(this_cpu,
+					      busiest->curr->cpus_ptr)) {
 				raw_spin_rq_unlock_irqrestore(busiest, flags);
 				goto out_one_pinned;
 			}
@@ -10119,7 +10728,8 @@ static int load_balance(int this_cpu, struct rq *this_rq,
 			raw_spin_rq_unlock_irqrestore(busiest, flags);
 
 			if (active_balance) {
-				stop_one_cpu_nowait(cpu_of(busiest),
+				stop_one_cpu_nowait(
+					cpu_of(busiest),
 					active_load_balance_cpu_stop, busiest,
 					&busiest->active_balance_work);
 			}
@@ -10179,8 +10789,8 @@ static int load_balance(int this_cpu, struct rq *this_rq,
 	return ld_moved;
 }
 
-static inline unsigned long
-get_sd_balance_interval(struct sched_domain *sd, int cpu_busy)
+static inline unsigned long get_sd_balance_interval(struct sched_domain *sd,
+						    int cpu_busy)
 {
 	unsigned long interval = sd->balance_interval;
 
@@ -10203,8 +10813,8 @@ get_sd_balance_interval(struct sched_domain *sd, int cpu_busy)
 	return interval;
 }
 
-static inline void
-update_next_balance(struct sched_domain *sd, unsigned long *next_balance)
+static inline void update_next_balance(struct sched_domain *sd,
+				       unsigned long *next_balance)
 {
 	unsigned long interval, next;
 
@@ -10259,20 +10869,21 @@ static int active_load_balance_cpu_stop(void *data)
 
 	/* Search for an sd spanning us and the target CPU. */
 	rcu_read_lock();
-	for_each_domain(target_cpu, sd) {
+	for_each_domain(target_cpu, sd)
+	{
 		if (cpumask_test_cpu(busiest_cpu, sched_domain_span(sd)))
 			break;
 	}
 
 	if (likely(sd)) {
 		struct lb_env env = {
-			.sd		= sd,
-			.dst_cpu	= target_cpu,
-			.dst_rq		= target_rq,
-			.src_cpu	= busiest_rq->cpu,
-			.src_rq		= busiest_rq,
-			.idle		= CPU_IDLE,
-			.flags		= LBF_ACTIVE_LB,
+			.sd = sd,
+			.dst_cpu = target_cpu,
+			.dst_rq = target_rq,
+			.src_cpu = busiest_rq->cpu,
+			.src_rq = busiest_rq,
+			.idle = CPU_IDLE,
+			.flags = LBF_ACTIVE_LB,
 		};
 
 		schedstat_inc(sd->alb_count);
@@ -10308,7 +10919,7 @@ static DEFINE_SPINLOCK(balancing);
  */
 void update_max_interval(void)
 {
-	max_load_balance_interval = HZ*num_online_cpus()/10;
+	max_load_balance_interval = HZ * num_online_cpus() / 10;
 }
 
 static inline bool update_newidle_cost(struct sched_domain *sd, u64 cost)
@@ -10349,13 +10960,14 @@ static void rebalance_domains(struct rq *rq, enum cpu_idle_type idle)
 	unsigned long interval;
 	struct sched_domain *sd;
 	/* Earliest time when we have to do rebalance again */
-	unsigned long next_balance = jiffies + 60*HZ;
+	unsigned long next_balance = jiffies + 60 * HZ;
 	int update_next_balance = 0;
 	int need_serialize, need_decay = 0;
 	u64 max_cost = 0;
 
 	rcu_read_lock();
-	for_each_domain(cpu, sd) {
+	for_each_domain(cpu, sd)
+	{
 		/*
 		 * Decay the newidle max times here because this is a regular
 		 * visit to all the domains.
@@ -10383,7 +10995,8 @@ static void rebalance_domains(struct rq *rq, enum cpu_idle_type idle)
 		}
 
 		if (time_after_eq(jiffies, sd->last_balance + interval)) {
-			if (load_balance(cpu, rq, sd, idle, &continue_balancing)) {
+			if (load_balance(cpu, rq, sd, idle,
+					 &continue_balancing)) {
 				/*
 				 * The LBF_DST_PINNED logic could have changed
 				 * env->dst_cpu, so we can't know our idle
@@ -10397,7 +11010,7 @@ static void rebalance_domains(struct rq *rq, enum cpu_idle_type idle)
 		}
 		if (need_serialize)
 			spin_unlock(&balancing);
-out:
+	out:
 		if (time_after(next_balance, sd->last_balance + interval)) {
 			next_balance = sd->last_balance + interval;
 			update_next_balance = 1;
@@ -10420,7 +11033,6 @@ static void rebalance_domains(struct rq *rq, enum cpu_idle_type idle)
 	 */
 	if (likely(update_next_balance))
 		rq->next_balance = next_balance;
-
 }
 
 static inline int on_null_domain(struct rq *rq)
@@ -10445,8 +11057,7 @@ static inline int find_new_ilb(void)
 
 	hk_mask = housekeeping_cpumask(HK_TYPE_MISC);
 
-	for_each_cpu_and(ilb, nohz.idle_cpus_mask, hk_mask) {
-
+	for_each_cpu_and (ilb, nohz.idle_cpus_mask, hk_mask) {
 		if (ilb == smp_processor_id())
 			continue;
 
@@ -10470,7 +11081,7 @@ static void kick_ilb(unsigned int flags)
 	 * not if we only update stats.
 	 */
 	if (flags & NOHZ_BALANCE_KICK)
-		nohz.next_balance = jiffies+1;
+		nohz.next_balance = jiffies + 1;
 
 	ilb_cpu = find_new_ilb();
 
@@ -10555,7 +11166,8 @@ static void nohz_balancer_kick(struct rq *rq)
 		 * currently idle; in which case, kick the ILB to move tasks
 		 * around.
 		 */
-		for_each_cpu_and(i, sched_domain_span(sd), nohz.idle_cpus_mask) {
+		for_each_cpu_and (i, sched_domain_span(sd),
+				  nohz.idle_cpus_mask) {
 			if (sched_asym_prefer(i, cpu)) {
 				flags = NOHZ_STATS_KICK | NOHZ_BALANCE_KICK;
 				goto unlock;
@@ -10746,7 +11358,7 @@ static void _nohz_idle_balance(struct rq *this_rq, unsigned int flags,
 {
 	/* Earliest time when we have to do rebalance again */
 	unsigned long now = jiffies;
-	unsigned long next_balance = now + 60*HZ;
+	unsigned long next_balance = now + 60 * HZ;
 	bool has_blocked_load = false;
 	int update_next_balance = 0;
 	int this_cpu = this_rq->cpu;
@@ -10780,7 +11392,7 @@ static void _nohz_idle_balance(struct rq *this_rq, unsigned int flags,
 	 * Start with the next CPU after this_cpu so we will end with this_cpu and let a
 	 * chance for other idle cpu to pull load.
 	 */
-	for_each_cpu_wrap(balance_cpu,  nohz.idle_cpus_mask, this_cpu+1) {
+	for_each_cpu_wrap (balance_cpu, nohz.idle_cpus_mask, this_cpu + 1) {
 		if (!idle_cpu(balance_cpu))
 			continue;
 
@@ -10908,14 +11520,19 @@ static void nohz_newidle_balance(struct rq *this_rq)
 }
 
 #else /* !CONFIG_NO_HZ_COMMON */
-static inline void nohz_balancer_kick(struct rq *rq) { }
+static inline void nohz_balancer_kick(struct rq *rq)
+{
+}
 
-static inline bool nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle)
+static inline bool nohz_idle_balance(struct rq *this_rq,
+				     enum cpu_idle_type idle)
 {
 	return false;
 }
 
-static inline void nohz_newidle_balance(struct rq *this_rq) { }
+static inline void nohz_newidle_balance(struct rq *this_rq)
+{
+}
 #endif /* CONFIG_NO_HZ_COMMON */
 
 /*
@@ -10969,7 +11586,6 @@ static int newidle_balance(struct rq *this_rq, struct rq_flags *rf)
 
 	if (!READ_ONCE(this_rq->rd->overload) ||
 	    (sd && this_rq->avg_idle < sd->max_newidle_lb_cost)) {
-
 		if (sd)
 			update_next_balance(sd, &next_balance);
 		rcu_read_unlock();
@@ -10984,7 +11600,8 @@ static int newidle_balance(struct rq *this_rq, struct rq_flags *rf)
 	update_blocked_averages(this_cpu);
 
 	rcu_read_lock();
-	for_each_domain(this_cpu, sd) {
+	for_each_domain(this_cpu, sd)
+	{
 		int continue_balancing = 1;
 		u64 domain_cost;
 
@@ -10994,9 +11611,8 @@ static int newidle_balance(struct rq *this_rq, struct rq_flags *rf)
 			break;
 
 		if (sd->flags & SD_BALANCE_NEWIDLE) {
-
-			pulled_task = load_balance(this_cpu, this_rq,
-						   sd, CPU_NEWLY_IDLE,
+			pulled_task = load_balance(this_cpu, this_rq, sd,
+						   CPU_NEWLY_IDLE,
 						   &continue_balancing);
 
 			t1 = sched_clock_cpu(this_cpu);
@@ -11056,8 +11672,8 @@ static int newidle_balance(struct rq *this_rq, struct rq_flags *rf)
 static __latent_entropy void run_rebalance_domains(struct softirq_action *h)
 {
 	struct rq *this_rq = this_rq();
-	enum cpu_idle_type idle = this_rq->idle_balance ?
-						CPU_IDLE : CPU_NOT_IDLE;
+	enum cpu_idle_type idle =
+		this_rq->idle_balance ? CPU_IDLE : CPU_NOT_IDLE;
 
 	/*
 	 * If this CPU has a pending nohz_balance_kick, then do the
@@ -11111,8 +11727,8 @@ static void rq_offline_fair(struct rq *rq)
 #endif /* CONFIG_SMP */
 
 #ifdef CONFIG_SCHED_CORE
-static inline bool
-__entity_slice_used(struct sched_entity *se, int min_nr_tasks)
+static inline bool __entity_slice_used(struct sched_entity *se,
+				       int min_nr_tasks)
 {
 	u64 slice = sched_slice(cfs_rq_of(se), se);
 	u64 rtime = se->sum_exec_runtime - se->prev_sum_exec_runtime;
@@ -11120,7 +11736,7 @@ __entity_slice_used(struct sched_entity *se, int min_nr_tasks)
 	return (rtime * min_nr_tasks > slice);
 }
 
-#define MIN_NR_TASKS_DURING_FORCEIDLE	2
+#define MIN_NR_TASKS_DURING_FORCEIDLE 2
 static inline void task_tick_core(struct rq *rq, struct task_struct *curr)
 {
 	if (!sched_core_enabled(rq))
@@ -11148,9 +11764,11 @@ static inline void task_tick_core(struct rq *rq, struct task_struct *curr)
 /*
  * se_fi_update - Update the cfs_rq->min_vruntime_fi in a CFS hierarchy if needed.
  */
-static void se_fi_update(struct sched_entity *se, unsigned int fi_seq, bool forceidle)
+static void se_fi_update(struct sched_entity *se, unsigned int fi_seq,
+			 bool forceidle)
 {
-	for_each_sched_entity(se) {
+	for_each_sched_entity(se)
+	{
 		struct cfs_rq *cfs_rq = cfs_rq_of(se);
 
 		if (forceidle) {
@@ -11220,7 +11838,9 @@ bool cfs_prio_less(struct task_struct *a, struct task_struct *b, bool in_fi)
 	return delta > 0;
 }
 #else
-static inline void task_tick_core(struct rq *rq, struct task_struct *curr) {}
+static inline void task_tick_core(struct rq *rq, struct task_struct *curr)
+{
+}
 #endif
 
 /*
@@ -11236,7 +11856,8 @@ static void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)
 	struct cfs_rq *cfs_rq;
 	struct sched_entity *se = &curr->se;
 
-	for_each_sched_entity(se) {
+	for_each_sched_entity(se)
+	{
 		cfs_rq = cfs_rq_of(se);
 		entity_tick(cfs_rq, se, queued);
 	}
@@ -11290,8 +11911,7 @@ static void task_fork_fair(struct task_struct *p)
  * Priority of the task has changed. Check to see if we preempt
  * the current task.
  */
-static void
-prio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio)
+static void prio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio)
 {
 	if (!task_on_rq_queued(p))
 		return;
@@ -11353,10 +11973,11 @@ static void propagate_entity_cfs_rq(struct sched_entity *se)
 	/* Start to propagate at parent */
 	se = se->parent;
 
-	for_each_sched_entity(se) {
+	for_each_sched_entity(se)
+	{
 		cfs_rq = cfs_rq_of(se);
 
-		if (!cfs_rq_throttled(cfs_rq)){
+		if (!cfs_rq_throttled(cfs_rq)) {
 			update_load_avg(cfs_rq, se, UPDATE_TG);
 			list_add_leaf_cfs_rq(cfs_rq);
 			continue;
@@ -11367,7 +11988,9 @@ static void propagate_entity_cfs_rq(struct sched_entity *se)
 	}
 }
 #else
-static void propagate_entity_cfs_rq(struct sched_entity *se) { }
+static void propagate_entity_cfs_rq(struct sched_entity *se)
+{
+}
 #endif
 
 static void detach_entity_cfs_rq(struct sched_entity *se)
@@ -11394,7 +12017,8 @@ static void attach_entity_cfs_rq(struct sched_entity *se)
 #endif
 
 	/* Synchronize entity with its cfs_rq */
-	update_load_avg(cfs_rq, se, sched_feat(ATTACH_AGE_LOAD) ? 0 : SKIP_AGE_LOAD);
+	update_load_avg(cfs_rq, se,
+			sched_feat(ATTACH_AGE_LOAD) ? 0 : SKIP_AGE_LOAD);
 	attach_entity_load_avg(cfs_rq, se);
 	update_tg_load_avg(cfs_rq);
 	propagate_entity_cfs_rq(se);
@@ -11469,7 +12093,8 @@ static void set_next_task_fair(struct rq *rq, struct task_struct *p, bool first)
 	}
 #endif
 
-	for_each_sched_entity(se) {
+	for_each_sched_entity(se)
+	{
 		struct cfs_rq *cfs_rq = cfs_rq_of(se);
 
 		set_next_entity(cfs_rq, se);
@@ -11528,7 +12153,7 @@ void free_fair_sched_group(struct task_group *tg)
 {
 	int i;
 
-	for_each_possible_cpu(i) {
+	for_each_possible_cpu (i) {
 		if (tg->cfs_rq)
 			kfree(tg->cfs_rq[i]);
 		if (tg->se)
@@ -11556,14 +12181,14 @@ int alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)
 
 	init_cfs_bandwidth(tg_cfs_bandwidth(tg));
 
-	for_each_possible_cpu(i) {
-		cfs_rq = kzalloc_node(sizeof(struct cfs_rq),
-				      GFP_KERNEL, cpu_to_node(i));
+	for_each_possible_cpu (i) {
+		cfs_rq = kzalloc_node(sizeof(struct cfs_rq), GFP_KERNEL,
+				      cpu_to_node(i));
 		if (!cfs_rq)
 			goto err;
 
-		se = kzalloc_node(sizeof(struct sched_entity_stats),
-				  GFP_KERNEL, cpu_to_node(i));
+		se = kzalloc_node(sizeof(struct sched_entity_stats), GFP_KERNEL,
+				  cpu_to_node(i));
 		if (!se)
 			goto err_free_rq;
 
@@ -11587,7 +12212,7 @@ void online_fair_sched_group(struct task_group *tg)
 	struct rq *rq;
 	int i;
 
-	for_each_possible_cpu(i) {
+	for_each_possible_cpu (i) {
 		rq = cpu_rq(i);
 		se = tg->se[i];
 		rq_lock_irq(rq, &rf);
@@ -11606,7 +12231,7 @@ void unregister_fair_sched_group(struct task_group *tg)
 
 	destroy_cfs_bandwidth(tg_cfs_bandwidth(tg));
 
-	for_each_possible_cpu(cpu) {
+	for_each_possible_cpu (cpu) {
 		if (tg->se[cpu])
 			remove_entity_load_avg(tg->se[cpu]);
 
@@ -11626,8 +12251,8 @@ void unregister_fair_sched_group(struct task_group *tg)
 }
 
 void init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq,
-			struct sched_entity *se, int cpu,
-			struct sched_entity *parent)
+		       struct sched_entity *se, int cpu,
+		       struct sched_entity *parent)
 {
 	struct rq *rq = cpu_rq(cpu);
 
@@ -11676,7 +12301,7 @@ static int __sched_group_set_shares(struct task_group *tg, unsigned long shares)
 		return 0;
 
 	tg->shares = shares;
-	for_each_possible_cpu(i) {
+	for_each_possible_cpu (i) {
 		struct rq *rq = cpu_rq(i);
 		struct sched_entity *se = tg->se[i];
 		struct rq_flags rf;
@@ -11684,7 +12309,8 @@ static int __sched_group_set_shares(struct task_group *tg, unsigned long shares)
 		/* Propagate contribution to hierarchy */
 		rq_lock_irqsave(rq, &rf);
 		update_rq_clock(rq);
-		for_each_sched_entity(se) {
+		for_each_sched_entity(se)
+		{
 			update_load_avg(cfs_rq_of(se), se, UPDATE_TG);
 			update_cfs_group(se);
 		}
@@ -11727,7 +12353,7 @@ int sched_group_set_idle(struct task_group *tg, long idle)
 
 	tg->idle = idle;
 
-	for_each_possible_cpu(i) {
+	for_each_possible_cpu (i) {
 		struct rq *rq = cpu_rq(i);
 		struct sched_entity *se = tg->se[i];
 		struct cfs_rq *parent_cfs_rq, *grp_cfs_rq = tg->cfs_rq[i];
@@ -11754,7 +12380,8 @@ int sched_group_set_idle(struct task_group *tg, long idle)
 		if (!cfs_rq_is_idle(grp_cfs_rq))
 			idle_task_delta *= -1;
 
-		for_each_sched_entity(se) {
+		for_each_sched_entity(se)
+		{
 			struct cfs_rq *cfs_rq = cfs_rq_of(se);
 
 			if (!se->on_rq)
@@ -11767,7 +12394,7 @@ int sched_group_set_idle(struct task_group *tg, long idle)
 				break;
 		}
 
-next_cpu:
+	next_cpu:
 		rq_unlock_irqrestore(rq, &rf);
 	}
 
@@ -11783,21 +12410,27 @@ int sched_group_set_idle(struct task_group *tg, long idle)
 
 #else /* CONFIG_FAIR_GROUP_SCHED */
 
-void free_fair_sched_group(struct task_group *tg) { }
+void free_fair_sched_group(struct task_group *tg)
+{
+}
 
 int alloc_fair_sched_group(struct task_group *tg, struct task_group *parent)
 {
 	return 1;
 }
 
-void online_fair_sched_group(struct task_group *tg) { }
+void online_fair_sched_group(struct task_group *tg)
+{
+}
 
-void unregister_fair_sched_group(struct task_group *tg) { }
+void unregister_fair_sched_group(struct task_group *tg)
+{
+}
 
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 
-
-static unsigned int get_rr_interval_fair(struct rq *rq, struct task_struct *task)
+static unsigned int get_rr_interval_fair(struct rq *rq,
+					 struct task_struct *task)
 {
 	struct sched_entity *se = &task->se;
 	unsigned int rr_interval = 0;
@@ -11817,47 +12450,47 @@ static unsigned int get_rr_interval_fair(struct rq *rq, struct task_struct *task
  */
 DEFINE_SCHED_CLASS(fair) = {
 
-	.enqueue_task		= enqueue_task_fair,
-	.dequeue_task		= dequeue_task_fair,
-	.yield_task		= yield_task_fair,
-	.yield_to_task		= yield_to_task_fair,
+	.enqueue_task = enqueue_task_fair,
+	.dequeue_task = dequeue_task_fair,
+	.yield_task = yield_task_fair,
+	.yield_to_task = yield_to_task_fair,
 
-	.check_preempt_curr	= check_preempt_wakeup,
+	.check_preempt_curr = check_preempt_wakeup,
 
-	.pick_next_task		= __pick_next_task_fair,
-	.put_prev_task		= put_prev_task_fair,
-	.set_next_task          = set_next_task_fair,
+	.pick_next_task = __pick_next_task_fair,
+	.put_prev_task = put_prev_task_fair,
+	.set_next_task = set_next_task_fair,
 
 #ifdef CONFIG_SMP
-	.balance		= balance_fair,
-	.pick_task		= pick_task_fair,
-	.select_task_rq		= select_task_rq_fair,
-	.migrate_task_rq	= migrate_task_rq_fair,
+	.balance = balance_fair,
+	.pick_task = pick_task_fair,
+	.select_task_rq = select_task_rq_fair,
+	.migrate_task_rq = migrate_task_rq_fair,
 
-	.rq_online		= rq_online_fair,
-	.rq_offline		= rq_offline_fair,
+	.rq_online = rq_online_fair,
+	.rq_offline = rq_offline_fair,
 
-	.task_dead		= task_dead_fair,
-	.set_cpus_allowed	= set_cpus_allowed_common,
+	.task_dead = task_dead_fair,
+	.set_cpus_allowed = set_cpus_allowed_common,
 #endif
 
-	.task_tick		= task_tick_fair,
-	.task_fork		= task_fork_fair,
+	.task_tick = task_tick_fair,
+	.task_fork = task_fork_fair,
 
-	.prio_changed		= prio_changed_fair,
-	.switched_from		= switched_from_fair,
-	.switched_to		= switched_to_fair,
+	.prio_changed = prio_changed_fair,
+	.switched_from = switched_from_fair,
+	.switched_to = switched_to_fair,
 
-	.get_rr_interval	= get_rr_interval_fair,
+	.get_rr_interval = get_rr_interval_fair,
 
-	.update_curr		= update_curr_fair,
+	.update_curr = update_curr_fair,
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
-	.task_change_group	= task_change_group_fair,
+	.task_change_group = task_change_group_fair,
 #endif
 
 #ifdef CONFIG_UCLAMP_TASK
-	.uclamp_enabled		= 1,
+	.uclamp_enabled = 1,
 #endif
 };
 
@@ -11881,7 +12514,7 @@ void show_numa_stats(struct task_struct *p, struct seq_file *m)
 
 	rcu_read_lock();
 	ng = rcu_dereference(p->numa_group);
-	for_each_online_node(node) {
+	for_each_online_node (node) {
 		if (p->numa_faults) {
 			tsf = p->numa_faults[task_faults_idx(NUMA_MEM, node, 0)];
 			tpf = p->numa_faults[task_faults_idx(NUMA_MEM, node, 1)];
@@ -11908,7 +12541,6 @@ __init void init_sched_fair_class(void)
 	zalloc_cpumask_var(&nohz.idle_cpus_mask, GFP_NOWAIT);
 #endif
 #endif /* SMP */
-
 }
 
 /*
@@ -11985,11 +12617,12 @@ int sched_trace_rq_cpu_capacity(struct rq *rq)
 {
 	return rq ?
 #ifdef CONFIG_SMP
-		rq->cpu_capacity
+		       rq->cpu_capacity
 #else
-		SCHED_CAPACITY_SCALE
+		       SCHED_CAPACITY_SCALE
 #endif
-		: -1;
+		       :
+		       -1;
 }
 EXPORT_SYMBOL_GPL(sched_trace_rq_cpu_capacity);
 
@@ -12005,6 +12638,6 @@ EXPORT_SYMBOL_GPL(sched_trace_rd_span);
 
 int sched_trace_rq_nr_running(struct rq *rq)
 {
-        return rq ? rq->nr_running : -1;
+	return rq ? rq->nr_running : -1;
 }
 EXPORT_SYMBOL_GPL(sched_trace_rq_nr_running);
-- 
2.34.1

